{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing dependecies and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_path = r'D:\\Datasets\\fight'\n",
    "actions = ['fighting','not_fighting']\n",
    "fighting_path = r'D:\\Datasets\\fight\\fighting'\n",
    "not_fighting_path = r'D:\\Datasets\\fight\\not_fighting'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main functions to process and extract keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distances(hands_dict,body_kp,head_kp):\n",
    "\n",
    "    # creating a dictionary of distances between each keypoint (except of the same object) in the keypoint_dict\n",
    "    dist_dict = {}\n",
    "    keyhead = head_kp.keys()\n",
    "    keysh = hands_dict.keys()\n",
    "    keysb = body_kp.keys()\n",
    "\n",
    "    # calculating distances between keypoints on each hand: left to left, right to left, left to right and right to right\n",
    "    for i,keyi in enumerate(keysh,start =1):\n",
    "        for j,keyj in enumerate(keysh,start =1):\n",
    "            if j>=i:\n",
    "                break\n",
    "            distll = calc_euclid_dist(hands_dict[keyi][0],hands_dict[keyj][0])\n",
    "            distlr = calc_euclid_dist(hands_dict[keyi][0],hands_dict[keyj][1])\n",
    "            distrl = calc_euclid_dist(hands_dict[keyi][1],hands_dict[keyj][0])\n",
    "            distrr = calc_euclid_dist(hands_dict[keyi][1],hands_dict[keyj][1])\n",
    "            dist_dict[f'{keyi}'+f'{keyj}'] = list([distll,distlr,distrl,distrr])\n",
    " \n",
    "    # calculating distances between hands and bodies\n",
    "    for i,keyi in enumerate(keysh,start =1):\n",
    "        for j,keyj in enumerate(keysb,start =1):\n",
    "            if j>=i:\n",
    "                break\n",
    "\n",
    "            distlb = calc_euclid_dist(hands_dict[keyi][0],body_kp[keyj])\n",
    "            \n",
    "            distrb = calc_euclid_dist(body_kp[keyj],hands_dict[keyi][1])\n",
    "\n",
    "            dist_dict[f'{keyi}'+f'{keyj}'].append(distlb)\n",
    "            dist_dict[f'{keyi}'+f'{keyj}'].append(distrb)\n",
    "                \n",
    "    for i,keyi in enumerate(keysb,start =1):\n",
    "        for j,keyj in enumerate(keysh,start =1):\n",
    "            if j>=i:\n",
    "                break\n",
    "\n",
    "            distlb = calc_euclid_dist(hands_dict[keyj][0],body_kp[keyi])\n",
    "            \n",
    "            distrb = calc_euclid_dist(body_kp[keyi],hands_dict[keyj][1])\n",
    "            \n",
    "\n",
    "            dist_dict[f'{keyi}'+f'{keyj}'].append(distlb)\n",
    "            dist_dict[f'{keyi}'+f'{keyj}'].append(distrb)\n",
    "\n",
    "\n",
    "    # calculating distances between hands and heads\n",
    "    for i,keyi in enumerate(keysh,start =1):\n",
    "        for j,keyj in enumerate(keyhead,start =1):\n",
    "            if j>=i:\n",
    "                break\n",
    "\n",
    "            distlh = calc_euclid_dist(hands_dict[keyi][0],head_kp[keyj])\n",
    "            \n",
    "            distrh = calc_euclid_dist(head_kp[keyj],hands_dict[keyi][1])\n",
    "            \n",
    "            dist_dict[f'{keyi}'+f'{keyj}'].append(distlh)\n",
    "            dist_dict[f'{keyi}'+f'{keyj}'].append(distrh)\n",
    "\n",
    "\n",
    "\n",
    "    for i,keyi in enumerate(keyhead,start =1):\n",
    "        for j,keyj in enumerate(keysh,start =1):\n",
    "            if j>=i:\n",
    "                break\n",
    "\n",
    "            distlh = calc_euclid_dist(hands_dict[keyj][0],head_kp[keyi])\n",
    "            \n",
    "            distrh = calc_euclid_dist(head_kp[keyi],hands_dict[keyj][1])\n",
    "            \n",
    " \n",
    "            dist_dict[f'{keyi}'+f'{keyj}'].append(distlh)\n",
    "            dist_dict[f'{keyi}'+f'{keyj}'].append(distrh)\n",
    "\n",
    "\n",
    "\n",
    "    # calculating distances between bodies\n",
    "    for i,keyi in enumerate(keysb,start =1):\n",
    "        for j,keyj in enumerate(keysb,start =1):\n",
    "            if j>=i:\n",
    "                break\n",
    "\n",
    "            distbb = calc_euclid_dist(body_kp[keyi],body_kp[keyj])\n",
    "\n",
    "            dist_dict[f'{keyi}'+f'{keyj}'].append(distbb)\n",
    "\n",
    "\n",
    "    # calculating distances between heads\n",
    "    for i,keyi in enumerate(keyhead,start =1):\n",
    "        for j,keyj in enumerate(keyhead,start =1):\n",
    "            if j>=i:\n",
    "                break\n",
    "\n",
    "            disthh = calc_euclid_dist(head_kp[keyi],head_kp[keyj])\n",
    "\n",
    "            dist_dict[f'{keyi}'+f'{keyj}'].append(disthh)\n",
    "    \n",
    "    return dist_dict\n",
    "\n",
    "def extract_hands_keypoints(results, threshold_class, threshold_keypoint):\n",
    "    # creating a dictionary to collect keypoints to each object id as dictionary key\n",
    "    existing_kp = {}\n",
    "    for result,i_d in zip(results[0],results[0].boxes.id):\n",
    "        # There results for bounding boxes, and confidence scores for general detect\n",
    "        x1, y1, x2, y2,_, conf_for_detect, class_id_detected = (result.boxes.data.tolist())[0]\n",
    "        # If the confidence score for general detect is lower than threshold, skip\n",
    "        if conf_for_detect < threshold_class:\n",
    "            continue\n",
    "        # keypoints\n",
    "        keys = (result.keypoints.data.tolist())[0]\n",
    "        xl_key, yl_key, confl = keys[9]\n",
    "        if confl > threshold_keypoint:\n",
    "           l = [int(xl_key),int(yl_key)]\n",
    "        else:\n",
    "            l = []\n",
    "        xr_key, yr_key, confr = keys[10]\n",
    "        if confr > threshold_keypoint:\n",
    "           r = [int(xr_key),int(yr_key)]\n",
    "        else:\n",
    "            r = []\n",
    "        hands_coords = list([l,r])\n",
    "        # Adding existing hand keypoints of an object in a frame to the dictionary   \n",
    "        existing_kp[int(i_d)] = hands_coords\n",
    "    return existing_kp\n",
    "\n",
    "def extract_body_keypoints(results,threshold_class, threshold_keypoint):\n",
    "    # creating a dictionary to collect keypoints to each object id as dictionary key\n",
    "    existing_kp = {}\n",
    "    for result,i_d in zip(results[0],results[0].boxes.id):\n",
    "        # There results for bounding boxes, and confidence scores for general detect\n",
    "        x1, y1, x2, y2,_, conf_for_detect, class_id_detected = (result.boxes.data.tolist())[0]\n",
    "        # If the confidence score for general detect is lower than threshold, skip\n",
    "        if conf_for_detect < threshold_class:\n",
    "            continue\n",
    "        # keypoints\n",
    "        keys = (result.keypoints.data.tolist())[0]\n",
    "        xl_key, yl_key, confl = keys[5]\n",
    "        xr_key, yr_key, confr = keys[6]\n",
    "        if (confl>threshold_keypoint) and (confr>threshold_keypoint):\n",
    "            # Adding existing hand keypoints of an object in a frame to the dictionary   \n",
    "            mid_point  = list([int((xr_key+xl_key)/2),int((yl_key+yr_key)/2)])\n",
    "            \n",
    "        else:\n",
    "            mid_point = []\n",
    "\n",
    "        existing_kp[int(i_d)] = mid_point\n",
    "\n",
    "    return existing_kp\n",
    "\n",
    "def extract_head_keypoints(results,threshold_class, threshold_keypoint):\n",
    "    # creating a dictionary to collect keypoints to each object id as dictionary key\n",
    "    existing_kp = {}\n",
    "    for result,i_d in zip(results[0],results[0].boxes.id):\n",
    "        # There results for bounding boxes, and confidence scores for general detect\n",
    "        x1, y1, x2, y2,_, conf_for_detect, class_id_detected = (result.boxes.data.tolist())[0]\n",
    "        # If the confidence score for general detect is lower than threshold, skip\n",
    "        if conf_for_detect < threshold_class:\n",
    "            continue\n",
    "        # keypoints\n",
    "        keys = (result.keypoints.data.tolist())[0]\n",
    "        xh_key, yh_key, confh = keys[0]\n",
    "        if confh>threshold_keypoint:\n",
    "            # Adding existing hand keypoints of an object in a frame to the dictionary   \n",
    "            mid_point  = list([int(xh_key),int(yh_key)])\n",
    "        else:\n",
    "            mid_point = []\n",
    "        existing_kp[int(i_d)] = mid_point\n",
    "    return existing_kp\n",
    "\n",
    "def extract_keypoints(results, threshold_class):\n",
    "    existing_kp = {}\n",
    "    for result,i_d in zip(results[0],results[0].boxes.id):\n",
    "        # There results for bounding boxes, and confidence scores for general detect\n",
    "        x1, y1, x2, y2,_, conf_for_detect, class_id_detected = (result.boxes.data.tolist())[0]\n",
    "        # If the confidence score for general detect is lower than threshold, skip\n",
    "        if conf_for_detect < threshold_class:\n",
    "            continue\n",
    "        # keypoints\n",
    "        keys = (result.keypoints.data.tolist())[0]\n",
    "        keyp_arr = list()\n",
    "        for key in keys:\n",
    "            keyp_arr.append(key)\n",
    "        # Adding existing hand keypoints of an object in a frame to the dictionary   \n",
    "        existing_kp[int(i_d)] = keyp_arr\n",
    "    return existing_kp\n",
    "\n",
    "def calc_kp_to_kp_dist(keypoints_dict):\n",
    "    # creating a dictionary of distances between each keypoint (except of the same object) in the keypoint_dict\n",
    "    dist_dict = {}\n",
    "    keys = keypoints_dict.keys()\n",
    "    # calculating distances between keypoints \n",
    "    for l,keyi in enumerate(keys,start =1):\n",
    "        for m,keyj in enumerate(keys,start =1):\n",
    "            if m>=l:\n",
    "                break  \n",
    "            for i,p1 in enumerate(keypoints_dict[keyi]):\n",
    "                for j,p2 in enumerate(keypoints_dict[keyj]):\n",
    "                    dist = calc_euclid_dist(p1,p2)\n",
    "                    dist_dict[f'{keyi}'+f'{keyj}'+f'{i}'+f'{j}'] = dist\n",
    "    return dist_dict\n",
    "\n",
    "def calc_euclid_dist(p1,p2):\n",
    "    if (len(p1)>0) and (len(p2)>0):\n",
    "        dist = int(math.sqrt((p1[0]-p2[0])*(p1[0]-p2[0]) + (p1[1]-p2[1])*(p1[1]-p2[1])))\n",
    "        return dist\n",
    "    else: \n",
    "        return np.nan\n",
    "    \n",
    "def calc_grad(dist_dict):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing dictionaries and parametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"No suspicious activity\"\n",
    "text1 = \"Suspicious activity\"\n",
    "text3 = \"No people in sight\"\n",
    "color2 = (100, 200, 0)\n",
    "color1 = (100, 0, 200)\n",
    "color3 = (100, 100, 100)\n",
    "font_scale = 1.6\n",
    "thickness = 2\n",
    "\n",
    "winsize = 40\n",
    "all_keypoints = {}\n",
    "distance_dict = {}\n",
    "average_dist = {}\n",
    "grad_dict = {}\n",
    "outputs = [0,1]\n",
    "nums_sequences = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing YOLOv8 pose model and caption from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "modely = YOLO('yolov8l-pose.pt')  # load a pretrained YOLOv8n classification model\n",
    "modely.to(device)\n",
    "video_path = r\"D:\\videos\\fight4.mp4\"\n",
    "vid_name = 'v100'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "# Get video properties\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) # or number\n",
    "# Create a VideoWriter object to save the output video\n",
    "output_video_path = r\"D:\\videos_processed\\fight4_processed.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data coleccting cycle. Each (winsize) frames you will have to press 'f' key if fight was on those frames and any other key if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pair 41.\n",
      "Processing pair 42.\n",
      "Processing pair 92.\n",
      "Processing pair 12.\n",
      "Processing pair 12.\n",
      "Processing pair 21.\n"
     ]
    }
   ],
   "source": [
    "while cap.isOpened():\n",
    "# Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "\n",
    "        results = modely.track(frame, persist=True, retina_masks=True, show_boxes=True, show_conf=False, line_width=1,  conf=0.8, iou=0.5,  classes=0, show_labels=False, device=device,verbose = False,tracker=\"bytetrack.yaml\")\n",
    "        if results[0].boxes.id is not None:\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)\n",
    "            ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
    "            annotated_frame = results[0].plot(probs=False, masks=True, boxes=True, line_width=1)\n",
    "            #extracting keypoints\n",
    "            body_kp = extract_body_keypoints(results = results,threshold_class=0.4,threshold_keypoint=0.4)\n",
    "            hands_kp = extract_hands_keypoints(results = results,threshold_class=0.4,threshold_keypoint=0.4)\n",
    "            head_kp = extract_head_keypoints(results = results,threshold_class=0.4,threshold_keypoint=0.4)\n",
    "            #calculating distances between keypoints\n",
    "\n",
    "            dd = calc_distances(hands_kp,body_kp,head_kp)\n",
    "            #appending distances dictionary and evaluating average distance and classification based on it\n",
    "            for key in dd.keys():\n",
    "\n",
    "                if key not in distance_dict.keys():\n",
    "                    distance_dict[key] = deque(maxlen=40)\n",
    "\n",
    "                distance_dict[key].append(dd[key])\n",
    "                \n",
    "                if len(distance_dict[key]) == winsize:\n",
    "                    nums_sequences = nums_sequences + 1\n",
    "                    print(f'Processing pair {key}.')\n",
    "                    keypoints = np.array(distance_dict[key])\n",
    "                    if cv2.waitKey(-1) & 0xFF == ord('f'):\n",
    "                        if cv2.waitKey(-1) & 0xFF == ord('f'):\n",
    "                            save_path = fighting_path   + f'\\{vid_name}' +  f'{nums_sequences}'\n",
    "                        else:\n",
    "                            save_path = not_fighting_path  + f'\\{vid_name}' +  f'{nums_sequences}'\n",
    "                    else:\n",
    "                        distance_dict[key].clear()\n",
    "                        continue\n",
    "                    np.save(save_path,keypoints)\n",
    "                    distance_dict[key].clear()\n",
    "            \n",
    "\n",
    "            annotated_frame_show = cv2.resize(annotated_frame, (1080, 720))\n",
    "            cv2.imshow(\"YOLOv8 Inference\", annotated_frame_show)\n",
    "            # Break the loop if 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import torch\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sequence(seq):\n",
    "    mean = torch.mean(seq)\n",
    "    std_dev = torch.std(seq) + 0.0001\n",
    "    standardized_data = (seq - mean) / std_dev\n",
    "    return standardized_data\n",
    "\n",
    "#data = torch.Tensor([ np.nan,  np.nan,  np.nan,  np.nan,  np.nan,  np.nan,  np.nan,  np.nan,  np.nan,  \n",
    "  #                   np.nan])\n",
    "def approximate_linear(series):\n",
    "    i_0 = 0\n",
    "    length = series.shape[0]\n",
    "    left_value = 0\n",
    "    right_value = 0\n",
    "    for i,num in enumerate(series):\n",
    "        if torch.isnan(num) == True:\n",
    "            i_0 = i\n",
    "            for j in range(i_0,length):\n",
    "                if torch.isnan(series[j]) == False:\n",
    "                    right_value  = series[j]\n",
    "                    if (i_0) == 0:\n",
    "                        left_value = right_value \n",
    "                    else:\n",
    "                        left_value = series[i_0 - 1]\n",
    "                    \n",
    "                    tg = (right_value - left_value)/(j - i_0 + 1)\n",
    "                    for k,ind  in enumerate(range(i_0,j)):\n",
    "                        series[ind] = left_value + (k+1)*tg\n",
    "                    break\n",
    "                    \n",
    "                else:\n",
    "                    if ((j+1) == length)&(i_0 == 0):\n",
    "                        series = torch.nan_to_num(series,0)\n",
    "                        return series\n",
    "                    if (j+1) == length:\n",
    "                        for k,ind  in enumerate(range(i_0,j+1)):\n",
    "                            series[ind] = left_value\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return normalize_sequence(series)\n",
    "\n",
    "\n",
    "def approx_gaps(sequences):\n",
    "    seq_data = torch.transpose(torch.Tensor(sequences),1,2)\n",
    "    data_shape = seq_data.shape\n",
    "    output_data = torch.zeros((data_shape)) \n",
    "    for i,seq in enumerate(seq_data):\n",
    "        for j,series in enumerate(seq):\n",
    "            output_data[i][j] = approximate_linear(series)\n",
    "\n",
    "    return output_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([536, 14, 40])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.7817, 0.2183])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {label: num for num, label in enumerate(actions)}\n",
    "sequences,labels = [],[]\n",
    "for action in actions:\n",
    "    for file in os.listdir(os.path.join(DATA_path,action)):\n",
    "        sequences.append(np.load(os.path.join(DATA_path,action,file),allow_pickle=True))\n",
    "        labels.append(label_map[action])\n",
    "\n",
    "seq_data = approx_gaps(sequences=np.array(sequences))\n",
    "\n",
    "\n",
    "seq_labels = to_categorical(labels).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(seq_data,seq_labels,test_size=0.1) \n",
    "\n",
    "\n",
    "num0class = np.sum(np.argmax(y_train,axis = 1) ==0 )\n",
    "num1class = np.sum(np.argmax(y_train,axis = 1) ==1 )\n",
    "\n",
    "num01class = len(y_train)\n",
    "print(X_train.shape)\n",
    "loss_weights = torch.Tensor([1 - num0class/num01class,1 - num1class/num01class])\n",
    "loss_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfering data to pytorch compatible type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keypoint_sequence_dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.shape = x.shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx, :], self.y[idx]\n",
    "        \n",
    "train_data = Keypoint_sequence_dataset(X_train,y_train)\n",
    "test_data = Keypoint_sequence_dataset(X_test,y_test)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building neural network and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import wandb\n",
    "from filelock import FileLock\n",
    "from ray import train, tune\n",
    "from ray.train import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "#wandb.login(key = '66d1d2eaf7cd3f83b644fc151071bbf5d7f0c237')\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto generated network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layers_constructor(in_channels,out_dim,activation_function = nn.ReLU,num_layers = 1,kernel_size = 3,channel_scale = 2,dropout = 0):\n",
    "    \n",
    "    conv_layers = nn.Sequential()\n",
    "    out_channels = int(in_channels*channel_scale)\n",
    "    conv_layers.append(nn.Conv1d(in_channels=in_channels,\n",
    "                                 out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size,\n",
    "                                 padding='same'))\n",
    "    conv_layers.append(activation_function())\n",
    "    conv_layers.append(nn.BatchNorm1d(out_channels))\n",
    "    conv_layers.append(nn.Dropout1d(dropout))\n",
    "\n",
    "    for layer in range(num_layers-2):\n",
    "        in_channels = out_channels\n",
    "        out_channels = int(in_channels*channel_scale)\n",
    "        conv_layers.append(nn.Conv1d(in_channels=in_channels,\n",
    "                                    out_channels=out_channels,\n",
    "                                    kernel_size=kernel_size,\n",
    "                                    padding='same'))\n",
    "        conv_layers.append(activation_function())\n",
    "        conv_layers.append(nn.BatchNorm1d(out_channels))\n",
    "        conv_layers.append(nn.Dropout1d(dropout))\n",
    "    in_channels = out_channels\n",
    "    out_channels = int(out_dim)\n",
    "    conv_layers.append(nn.Conv1d(in_channels=in_channels,\n",
    "                                    out_channels=out_dim,\n",
    "                                    kernel_size=kernel_size,\n",
    "                                    padding='same'))\n",
    "    conv_layers.append(activation_function())\n",
    "    conv_layers.append(nn.BatchNorm1d(out_channels))\n",
    "    conv_layers.append(nn.Dropout1d(dropout))    \n",
    "    return conv_layers,out_channels\n",
    "\n",
    "def dense_layers_constructor(in_channels,activation_function = nn.ReLU,num_layers = 1,channel_scale = 2,dropout = 0):\n",
    "    \n",
    "    dense_layers = nn.Sequential()\n",
    "    out_channels = int(in_channels*channel_scale)\n",
    "    dense_layers.append(nn.Linear(in_features=in_channels,out_features=out_channels))\n",
    "    dense_layers.append(activation_function())\n",
    "    dense_layers.append(nn.BatchNorm1d(out_channels))\n",
    "    dense_layers.append(nn.Dropout1d(dropout))\n",
    "    \n",
    "    for layer in range(num_layers-1):\n",
    "        in_channels = out_channels\n",
    "        out_channels = int(in_channels*channel_scale)\n",
    "        dense_layers.append(nn.Linear(in_features=in_channels,out_features=out_channels))\n",
    "        dense_layers.append(activation_function())\n",
    "        dense_layers.append(nn.BatchNorm1d(out_channels))\n",
    "        dense_layers.append(nn.Dropout1d(dropout))\n",
    "        \n",
    "    return dense_layers,out_channels\n",
    "\n",
    "def logit_layer(in_channels,out_channels):\n",
    "    logit_layers = nn.Sequential()\n",
    "    logit_layers.append(nn.Linear(in_channels,out_channels))\n",
    "    logit_layers.append(nn.Softmax(1))\n",
    "    return logit_layers\n",
    "\n",
    "class Auto_conv_net(nn.Module):\n",
    "    def __init__(self,kernel_size = 3,dropout_conv = 0.3,\n",
    "                 dropout_linear = 0.3,conv_out_dim = 14,\n",
    "                 attention_heads = 4,attention_dropout = 0.3,\n",
    "                 num_layers_conv = 2,channel_scale_conv = 2,\n",
    "                 max_pool_size = 10,num_layers_dense = 2,\n",
    "                 channel_scale_dense = 0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        conv_layers,conv_channels = conv_layers_constructor(14,conv_out_dim,activation_function=nn.GELU,\n",
    "                                                            num_layers=num_layers_conv,\n",
    "                                                            kernel_size=kernel_size,\n",
    "                                                            channel_scale=channel_scale_conv,\n",
    "                                                            dropout=dropout_conv)\n",
    "        self.conv_layers = conv_layers\n",
    "        self.maxpool1 = nn.AdaptiveMaxPool1d(max_pool_size)\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(conv_channels ,attention_heads,batch_first=True,dropout= attention_dropout)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        dense_layers,dense_channels = dense_layers_constructor(conv_channels*max_pool_size,activation_function=nn.GELU,\n",
    "                                                num_layers=num_layers_dense,\n",
    "                                                channel_scale=channel_scale_dense,\n",
    "                                                dropout = dropout_linear)\n",
    "        self.linear = dense_layers\n",
    "        self.output_layer = logit_layer(dense_channels,2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # forward pass сети\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = torch.transpose(x,1,2)\n",
    "        #x,_ = self.lstm1(x)\n",
    "        x,_ = self.attention(x,x,x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hand made network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq_conv_net(nn.Module):\n",
    "    def __init__(self,kernel_size = 3,dropout_conv = 0.3,\n",
    "                 dropout_linear = 0.3,\n",
    "                 attention_heads = 4,attention_dropout = 0.3,\n",
    "                 num_layers_conv = 2,channel_scale_conv = 2,\n",
    "                 max_pool_size = 10,num_layers_dense = 2,\n",
    "                 channel_scale_dense = 0.5,attention_size = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=14, out_channels=32, kernel_size =kernel_size,padding= 'same') ,\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout1d(dropout_conv)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size = kernel_size,padding= 'same') ,\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout1d(dropout_conv)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size = kernel_size,padding= 'same') ,\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout1d(dropout_conv)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128, out_channels=attention_size, kernel_size = kernel_size,padding= 'same') ,\n",
    "            nn.BatchNorm1d(attention_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout1d(dropout_conv)\n",
    "        )\n",
    "        self.maxpool1 = nn.AdaptiveMaxPool1d(20)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "       #self.lstm1 = nn.LSTM(64,64,LSTM_size,batch_first=True,dropout= LSTM_dropout)\n",
    "        self.attention = nn.MultiheadAttention(attention_size,attention_heads,batch_first=True,dropout= attention_dropout)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(attention_size*20, attention_size*5) ,\n",
    "            nn.BatchNorm1d(attention_size*5),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout1d(dropout_linear)\n",
    "        )\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.Linear(attention_size*5, attention_size) ,\n",
    "            nn.BatchNorm1d(attention_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout1d(dropout_linear)\n",
    "        )\n",
    "        self.linear3 = nn.Sequential(\n",
    "            nn.Linear(attention_size, 128) ,\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout1d(dropout_linear)\n",
    "        )\n",
    "        self.logits = nn.Sequential(\n",
    "            nn.Linear(128, 2) ,\n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # forward pass сети\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = torch.transpose(x,1,2)\n",
    "        #x,_ = self.lstm1(x)\n",
    "        x,_ = self.attention(x,x,x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.logits(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, loss_fn):\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    num_elements = len(dataloader.dataset)\n",
    "    num_correct = 0\n",
    "    num0_true = 0\n",
    "    num0_false = 0\n",
    "    num1_true = 0\n",
    "    num1_false = 0\n",
    "    num0_ = 0\n",
    "    num1_ = 0\n",
    "\n",
    "    model.train(False)\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        # так получаем текущий батч\n",
    "        X_batch, y_batch = batch\n",
    "        with torch.no_grad():\n",
    "            logits = model(X_batch.to(device,dtype = torch.float))\n",
    "            \n",
    "            loss = loss_fn(logits, y_batch.to(device,dtype = torch.float))\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            y_pred = torch.argmax(logits, dim=1).cpu()\n",
    "            \n",
    "            y_answers = torch.argmax(y_batch, dim=1).cpu()\n",
    "\n",
    "            num_correct += torch.sum(y_answers == y_pred)\n",
    "            num0_true += torch.sum((y_answers == 0)&(y_pred == 0))\n",
    "            num0_false += torch.sum((y_answers == 0)&(y_pred == 1))\n",
    "            num1_true += torch.sum((y_answers == 1)&(y_pred == 1))\n",
    "            num1_false += torch.sum((y_answers == 1)&(y_pred == 0))\n",
    "            num0_ +=torch.sum(y_answers == 0)\n",
    "            num1_ +=torch.sum(y_answers == 1)\n",
    "    accuracy = num_correct / num_elements   \n",
    "\n",
    "    f1 = F1_score(num0_true,num0_false,num1_false)\n",
    "\n",
    "    conf_matrix = np.array([[num1_true,num0_false],[num1_false,num0_true]])\n",
    "    \n",
    "    return float(accuracy), float(f1),float(np.mean(losses)), conf_matrix\n",
    "\n",
    "def F1_score(tp,fp,fn):\n",
    "    return (2*tp/(2*tp + fp + fn))\n",
    "\n",
    "def train_model(model, loss_fn, optimizer,train_loader,val_loader, n_epoch=3,raytune_mode = False,wandb_mode = False):\n",
    "\n",
    "    num_iter = 0\n",
    "    # цикл обучения сети\n",
    "    for epoch in range(n_epoch):\n",
    "\n",
    "        \n",
    "\n",
    "        model.train(True)\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            # так получаем текущий батч\n",
    "            X_batch, y_batch = batch \n",
    "            \n",
    "            # forward pass (получение ответов на батч картинок)\n",
    "            logits = model(X_batch.to(device,dtype = torch.float)) \n",
    "            \n",
    "            # вычисление лосса от выданных сетью ответов и правильных ответов на батч\n",
    "            loss = loss_fn(logits, y_batch.to(device,dtype = torch.float)) \n",
    "            \n",
    "            \n",
    "            loss.backward() # backpropagation (вычисление градиентов)\n",
    "            optimizer.step() # обновление весов сети\n",
    "            optimizer.zero_grad() # обнуляем веса\n",
    "\n",
    "            num_iter += 1\n",
    "\n",
    "\n",
    "        # после каждой эпохи получаем метрику качества на валидационной выборке\n",
    "        model.train(False)\n",
    "        \n",
    "        val_accuracy,f1_v, val_loss,_ = evaluate_model(model, val_loader, loss_fn=loss_fn)\n",
    "        train_accuracy,f1_t, train_loss,_ = evaluate_model(model, train_loader, loss_fn=loss_fn)\n",
    "        \n",
    "        if wandb_mode == True:\n",
    "            wandb.log({\"Val/accuracy\": val_accuracy,\"val/f1_metric\": f1_v ,\n",
    "                   \"Val/loss\": val_loss,\"train/accuracy\": train_accuracy,\n",
    "                   \"train/loss\": train_loss,'train/f1_metric':f1_t})\n",
    "        \n",
    "        if ((epoch+1)%25 ==0) and (raytune_mode == True):\n",
    "            os.makedirs(\"checkpoint_models\", exist_ok=True)\n",
    "            torch.save(\n",
    "                        (model.state_dict(), optimizer.state_dict()), \"checkpoint_models/checkpoint.pt\")\n",
    "            checkpoint = Checkpoint.from_directory(\"checkpoint_models\")\n",
    "            train.report({\"loss\": val_loss, \"accuracy\": val_accuracy,\"f1\": f1_v}, checkpoint=checkpoint)\n",
    "        #if epoch%10 ==0:\n",
    "        #    print(\"Epoch:\", epoch)\n",
    "        #    \n",
    "        #    print('Loss/train', train_loss, epoch)\n",
    "        #    print(f'Accuracy/train{train_accuracy:0.6f}')\n",
    "        #    print('Loss/val', val_loss, epoch)\n",
    "        #    print(f'Accuracy/val{val_accuracy:0.6f}')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:t26grfda) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3fa48f986945c3a553eecda54b13d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Val/accuracy</td><td>█▃▃▁▄▂▄▅▂▂▁▁▅▁▁▃▄▅▁▃▄▃▄▄▅▄▃▃▂▂▂▄▄▄▄▄▂▅▆▅</td></tr><tr><td>Val/loss</td><td>▂▃▄▄▃▄▃▃▄▅▄▃▂▄▄█▃▄▅▂▂▅▃▂▄▃▃▁▄▃▄▃▃▃▃▃▅▄▂▂</td></tr><tr><td>train/accuracy</td><td>▂▂▂▁▃▃▃▃▃▃▃▅▅▄▄▅▆▆▆▆▇▇▆▇▇▇▇▇▇▇█▇▇▇▇▇▇███</td></tr><tr><td>train/f1_metric</td><td>▁▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▆▇▇▇▇▇▇▇▇▇▇██▇▇▇▇▇███</td></tr><tr><td>train/loss</td><td>█▇▆▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val/f1_metric</td><td>▁▄▆▆▇▆▇▇▆█▅▅█▆▆▆▇▇▃▇▇▇▄▄▅▄▇▄▆▆▆▇▇▄▇▇▆▅▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Val/accuracy</td><td>0.76364</td></tr><tr><td>Val/loss</td><td>0.16017</td></tr><tr><td>train/accuracy</td><td>0.97571</td></tr><tr><td>train/f1_metric</td><td>0.94495</td></tr><tr><td>train/loss</td><td>0.13057</td></tr><tr><td>val/f1_metric</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stilted-bee-143</strong> at: <a href='https://wandb.ai/ustimovi/fight_detection_module/runs/t26grfda' target=\"_blank\">https://wandb.ai/ustimovi/fight_detection_module/runs/t26grfda</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231126_230434-t26grfda\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:t26grfda). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\repositories\\Action_recognition_research\\Action_recognition_research\\wandb\\run-20231126_231353-sfljrjgf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ustimovi/fight_detection_module/runs/sfljrjgf' target=\"_blank\">fluent-cloud-144</a></strong> to <a href='https://wandb.ai/ustimovi/fight_detection_module' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ustimovi/fight_detection_module' target=\"_blank\">https://wandb.ai/ustimovi/fight_detection_module</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ustimovi/fight_detection_module/runs/sfljrjgf' target=\"_blank\">https://wandb.ai/ustimovi/fight_detection_module/runs/sfljrjgf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_size = int(len(train_data) * 1)\n",
    "\n",
    "val_size = len(train_data) - train_size\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [train_size, val_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "#val_loader = torch.utils.data.DataLoader(val_data, batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "kernel_size = 3\n",
    "dropout_conv = 0.1\n",
    "dropout_linear = 0.3\n",
    "attention_heads = 8\n",
    "attention_dropout = 0.3\n",
    "num_layers_conv = 4\n",
    "channel_scale_conv = 2\n",
    "max_pool_size = 20\n",
    "num_layers_dense = 2\n",
    "channel_scale_dense = 0.5\n",
    "\n",
    "conv_net = Auto_conv_net(kernel_size = kernel_size,dropout_conv = dropout_conv,\n",
    "                 dropout_linear = dropout_linear ,\n",
    "                 attention_heads = attention_heads,attention_dropout = attention_dropout,\n",
    "                 num_layers_conv = num_layers_conv,channel_scale_conv = channel_scale_conv,\n",
    "                 max_pool_size = max_pool_size,num_layers_dense = num_layers_dense,\n",
    "                 channel_scale_dense = channel_scale_dense)\n",
    "#conv_net = Seq_conv_net(attention_size=256)\n",
    "conv_net = conv_net.to(device)\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=loss_weights.to(device))\n",
    "\n",
    "# выбираем алгоритм оптимизации и learning_rate\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(conv_net.parameters(), lr=learning_rate)\n",
    "\n",
    "run = wandb.init(\n",
    "        # Set the project where this run will be logged\n",
    "        project=\"fight_detection_module\",\n",
    "        # Track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"learning_rate\": learning_rate,\n",
    "            'kernel_size':  kernel_size,\n",
    "            'dropout_conv': dropout_conv ,\n",
    "            'dropout_linear': dropout_linear ,\n",
    "            'Attention_dropout': attention_dropout,\n",
    "            'attention_heads': attention_heads})\n",
    "\n",
    "\n",
    "conv_net = train_model(model=conv_net,loss_fn=loss_fn,optimizer=optimizer,train_loader=train_loader,val_loader=test_loader,n_epoch=100,raytune_mode= False,wandb_mode= True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Tuning (AutoML analog in torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_hp = {\n",
    "    \"learning_rate\": tune.grid_search([0.0001]),\n",
    "    \"batch_size\": tune.grid_search([32]),\n",
    "    \"kernel_size\":tune.grid_search([3,5,7]),\n",
    "    \"dropout_conv\": tune.grid_search([0.,0.1,0.2]),\n",
    "    \"dropout_linear\": tune.grid_search([0.,0.1,0.2]),\n",
    "    \"attention_dropout\": tune.grid_search([0.,0.1,0.2]),\n",
    "    \"attention_heads\": tune.grid_search([8]),\n",
    "    \"channel_scale_dense\": tune.grid_search([0.5]),\n",
    "    \"num_layers_conv\": tune.grid_search([2,4,6]),\n",
    "    \"channel_scale_conv\": tune.grid_search([2]),\n",
    "    \"num_layers_dense\": tune.grid_search([2,4,6]),\n",
    "    \"max_pool_size\": tune.grid_search([20])\n",
    "    }\n",
    "from ray.tune.search.optuna import OptunaSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_hp = {\n",
    "    \"learning_rate\": tune.choice([0.001]),\n",
    "    \"batch_size\": tune.choice([32,64]),\n",
    "    \"kernel_size\":tune.choice([3,5,7,9]),\n",
    "    \"dropout_conv\": tune.uniform(0,0.5),\n",
    "    \"dropout_linear\": tune.uniform(0,0.5),\n",
    "    \"attention_dropout\": tune.uniform(0,0.5),\n",
    "    \"attention_heads\": tune.choice([2,4,8,16]),\n",
    "    \"channel_scale_dense\": tune.uniform(0.2,0.7),\n",
    "    \"num_layers_conv\": tune.randint(1,5),\n",
    "    \"channel_scale_conv\": tune.uniform(1.5,2.5),\n",
    "    \"num_layers_dense\": tune.randint(1,6),\n",
    "    \"max_pool_size\": tune.randint(10,40),\n",
    "    \"conv_out_dim\": tune.choice([32,64,128,256])\n",
    "    }\n",
    "def ray_train_model(config):\n",
    "    batch_size = config['batch_size']\n",
    "    kernel_size = config['kernel_size']\n",
    "    dropout_conv = config['dropout_conv']\n",
    "    dropout_linear = config['dropout_linear']\n",
    "    attention_dropout = config['attention_dropout']\n",
    "    attention_heads = config['attention_heads']\n",
    "    learning_rate = config['learning_rate']\n",
    "    channel_scale_dense = config['channel_scale_dense']\n",
    "    num_layers_conv = config['num_layers_conv']\n",
    "    channel_scale_conv = config['channel_scale_conv']\n",
    "    num_layers_dense = config['num_layers_dense']\n",
    "    max_pool_size = config['max_pool_size']\n",
    "    conv_out_dim = config['conv_out_dim']\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=int(batch_size), shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=int(batch_size), shuffle=True)\n",
    "\n",
    "    conv_net = Auto_conv_net(kernel_size = kernel_size,dropout_conv = dropout_conv,\n",
    "                 dropout_linear = dropout_linear ,conv_out_dim= conv_out_dim,\n",
    "                 attention_heads = attention_heads,attention_dropout = attention_dropout,\n",
    "                 num_layers_conv = num_layers_conv,channel_scale_conv = channel_scale_conv,\n",
    "                 max_pool_size = max_pool_size,num_layers_dense = num_layers_dense,\n",
    "                 channel_scale_dense = channel_scale_dense)\n",
    "    \n",
    "    conv_net = conv_net.to(device)\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight=loss_weights.to(device))\n",
    "    \n",
    "    optimizer = torch.optim.Adam(conv_net.parameters(), lr=learning_rate)\n",
    "\n",
    "    #loaded_checkpoint = train.get_checkpoint()\n",
    "    #if loaded_checkpoint:\n",
    "    #    with loaded_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
    "    #       model_state, optimizer_state = torch.load(os.path.join(loaded_checkpoint_dir, \"checkpoint.pt\"))\n",
    "    #    conv_net.load_state_dict(model_state)\n",
    "    #    optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    conv_net = train_model(model=conv_net,loss_fn=loss_fn,optimizer=optimizer,train_loader=train_loader,val_loader=test_loader,n_epoch=50,raytune_mode=True,wandb_mode = False)\n",
    "   \n",
    "def evaluate_best_model(net, dataset, loss_fn,best_result,config):\n",
    "\n",
    "    batch_size = config['batch_size']\n",
    "    kernel_size = config['kernel_size']\n",
    "    dropout_conv = config['dropout_conv']\n",
    "    dropout_linear = config['dropout_linear']\n",
    "    attention_dropout = config['attention_dropout']\n",
    "    attention_heads = config['attention_heads']\n",
    "    learning_rate = config['learning_rate']\n",
    "    channel_scale_dense = config['channel_scale_dense']\n",
    "    num_layers_conv = config['num_layers_conv']\n",
    "    channel_scale_conv = config['channel_scale_conv']\n",
    "    num_layers_dense = config['num_layers_dense']\n",
    "    max_pool_size = config['max_pool_size']\n",
    "    conv_out_dim = config['conv_out_dim']\n",
    "    model = net(kernel_size = kernel_size,dropout_conv = dropout_conv,\n",
    "                 dropout_linear = dropout_linear ,conv_out_dim = conv_out_dim ,\n",
    "                 attention_heads = attention_heads,attention_dropout = attention_dropout,\n",
    "                 num_layers_conv = num_layers_conv,channel_scale_conv = channel_scale_conv,\n",
    "                 max_pool_size = max_pool_size,num_layers_dense = num_layers_dense,\n",
    "                 channel_scale_dense = channel_scale_dense)\n",
    "    model.to(device)\n",
    "    checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "    model_state, optimizer_state = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(model_state)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=int(config[\"batch_size\"]), shuffle=False)\n",
    "    return evaluate_model(model,dataloader,loss_fn)\n",
    "\n",
    "def main(num_samples=1, max_num_epochs=100, gpus_per_trial=1):\n",
    "    \n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    \n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(ray_train_model),\n",
    "            resources={\"cpu\": 4, \"gpu\": gpus_per_trial}\n",
    "        ),\n",
    "        run_config=train.RunConfig(\n",
    "        name=\"fight-exp\",\n",
    "        local_dir=r\"D:\\ray_temp\",\n",
    "        checkpoint_config=train.CheckpointConfig(\n",
    "            checkpoint_score_attribute=\"loss\",\n",
    "            checkpoint_score_order=\"min\",\n",
    "            num_to_keep=5,\n",
    "        ),\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            search_alg=OptunaSearch(),\n",
    "            metric=\"loss\",\n",
    "            mode=\"min\",\n",
    "            scheduler=scheduler,\n",
    "            num_samples=num_samples,\n",
    "        ),\n",
    "        param_space=config_hp,\n",
    "    )\n",
    "\n",
    "    results = tuner.fit()\n",
    "\n",
    "    return results\n",
    "\n",
    "results = main(num_samples=400, max_num_epochs=100, gpus_per_trial=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'learning_rate': 0.001, 'batch_size': 32, 'kernel_size': 5, 'dropout_conv': 0.010150648522749578, 'dropout_linear': 0.1437469742864229, 'attention_dropout': 0.2537245790193981, 'attention_heads': 4, 'channel_scale_dense': 0.5123263220716681, 'num_layers_conv': 3, 'channel_scale_conv': 1.8111476884893922, 'num_layers_dense': 1, 'max_pool_size': 38, 'conv_out_dim': 32}\n",
      "Best trial final validation loss: 0.18654301762580872\n",
      "Best trial final validation accuracy: 0.8666666746139526\n",
      "Best trial f1 metric: 0.692307710647583\n",
      "[[43  5]\n",
      " [ 3  9]]\n",
      "0.8666666746139526\n",
      "0.692307710647583\n"
     ]
    }
   ],
   "source": [
    "best_result = results.get_best_result(\"accuracy\", \"max\")\n",
    "\n",
    "print(\"Best trial config: {}\".format(best_result.config))\n",
    "print(\"Best trial final validation loss: {}\".format(\n",
    "        best_result.metrics[\"loss\"]))\n",
    "print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_result.metrics[\"accuracy\"]))\n",
    "print(\"Best trial f1 metric: {}\".format(\n",
    "        best_result.metrics[\"f1\"]))\n",
    "    \n",
    "a,b,e,conf_matrix = evaluate_best_model(Auto_conv_net,test_data,torch.nn.CrossEntropyLoss(weight=loss_weights.to(device)),best_result,best_result.config)\n",
    "print(conf_matrix)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'learning_rate': 0.001, 'batch_size': 32, 'kernel_size': 5, 'dropout_conv': 0.20868167245149438, 'dropout_linear': 0.10686674435199432, 'attention_dropout': 0.3543545446238282, 'attention_heads': 2, 'channel_scale_dense': 0.38573715366716493, 'num_layers_conv': 3, 'channel_scale_conv': 1.9383662335311946, 'num_layers_dense': 1, 'max_pool_size': 23, 'conv_out_dim': 32}\n",
      "Best trial final validation loss: 0.1630440503358841\n",
      "Best trial final validation accuracy: 0.8500000238418579\n",
      "Best trial f1 metric: 0.7272727489471436\n",
      "[[361  16]\n",
      " [ 58 101]]\n",
      "0.861940324306488\n",
      "0.7318840622901917\n"
     ]
    }
   ],
   "source": [
    "best_result = results.get_best_result(\"loss\", \"min\")\n",
    "\n",
    "print(\"Best trial config: {}\".format(best_result.config))\n",
    "print(\"Best trial final validation loss: {}\".format(\n",
    "        best_result.metrics[\"loss\"]))\n",
    "print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_result.metrics[\"accuracy\"]))\n",
    "print(\"Best trial f1 metric: {}\".format(\n",
    "        best_result.metrics[\"f1\"]))\n",
    "    \n",
    "a,b,e,conf_matrix = evaluate_best_model(Auto_conv_net,train_data,torch.nn.CrossEntropyLoss(weight=loss_weights.to(device)),best_result,best_result.config)\n",
    "print(conf_matrix)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.800000011920929 0.6000000238418579 0.20314578711986542 [[39  5]\n",
      " [ 7  9]]\n"
     ]
    }
   ],
   "source": [
    "def train_best_model(net, test_data,train_data, loss_fn,best_result,config):\n",
    "\n",
    "    batch_size = config['batch_size']\n",
    "    kernel_size = config['kernel_size']\n",
    "    dropout_conv = config['dropout_conv']\n",
    "    dropout_linear = config['dropout_linear']\n",
    "    attention_dropout = config['attention_dropout']\n",
    "    attention_heads = config['attention_heads']\n",
    "    learning_rate = config['learning_rate']\n",
    "    channel_scale_dense = config['channel_scale_dense']\n",
    "    num_layers_conv = config['num_layers_conv']\n",
    "    channel_scale_conv = config['channel_scale_conv']\n",
    "    num_layers_dense = config['num_layers_dense']\n",
    "    max_pool_size = config['max_pool_size']\n",
    "    conv_out_dim = config['conv_out_dim']\n",
    "    model = net(kernel_size = kernel_size,dropout_conv = dropout_conv,\n",
    "                 dropout_linear = dropout_linear ,conv_out_dim = conv_out_dim ,\n",
    "                 attention_heads = attention_heads,attention_dropout = attention_dropout,\n",
    "                 num_layers_conv = num_layers_conv,channel_scale_conv = channel_scale_conv,\n",
    "                 max_pool_size = max_pool_size,num_layers_dense = num_layers_dense,\n",
    "                 channel_scale_dense = channel_scale_dense)\n",
    "    model.to(device)\n",
    "    checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "    model_state, optimizer_state = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(model_state)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    learning_rate = 1e-4\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    \n",
    "\n",
    "    conv_net = train_model(model=model,loss_fn=loss_fn,optimizer=optimizer,train_loader=train_loader,val_loader=test_loader,n_epoch=500,raytune_mode= False,wandb_mode= False)\n",
    "\n",
    "    return evaluate_model(conv_net,test_loader,loss_fn)\n",
    "\n",
    "a,b,c,d = train_best_model(Auto_conv_net,test_data,train_data,torch.nn.CrossEntropyLoss(weight=loss_weights.to(device)),best_result,best_result.config)\n",
    "print(a,b,c,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scripted = torch.jit.script(conv_net)\n",
    "model_scripted.save('fight_detection_v2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on a real video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_keypoints(keypoints):\n",
    "    seq_data = torch.transpose(torch.Tensor(keypoints),0,1)\n",
    "    data_shape = seq_data.shape\n",
    "    output_data = np.zeros((data_shape)) \n",
    "    for i,seq in enumerate(seq_data):\n",
    "        output_data[i] = approximate_linear(seq)\n",
    "\n",
    "    return torch.Tensor([output_data])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "modely = YOLO('yolov8l-pose.pt')  # load a pretrained YOLOv8n classification model\n",
    "modely.to(device)\n",
    "video_path = r\"D:\\videos\\hands3.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "# Get video properties\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) # or number\n",
    "# Create a VideoWriter object to save the output video\n",
    "output_video_path = r\"D:\\videos_processed\\fight1_processed.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "\n",
    "actions = ['fighting','not_fighting']\n",
    "\n",
    "text2 = \"No suspicious activity\"\n",
    "text1 = \"Suspicious activity\"\n",
    "text3 = \"No people in sight\"\n",
    "color_map = {'fighting': (200,100,0),'not_fighting': (0,100,200)}\n",
    "font_scale = 1.6\n",
    "thickness = 2\n",
    "\n",
    "winsize = 40\n",
    "\n",
    "distance_dict = {}\n",
    "\n",
    "\n",
    "label_map = {num: label for num, label in enumerate(actions)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1317, 0.8683]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0314, 0.9686]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0830, 0.9170]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.6340, 0.3660]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0755, 0.9245]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0766, 0.9234]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.6932, 0.3068]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0782, 0.9218]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3580, 0.6420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2321, 0.7679]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4379, 0.5621]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1167, 0.8833]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.8522, 0.1478]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.6053, 0.3947]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2368, 0.7632]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.9202, 0.0798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.6202, 0.3798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.7120, 0.2880]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0434, 0.9566]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.7264, 0.2736]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ans = 'not_fighting'\n",
    "while cap.isOpened():\n",
    "# Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "\n",
    "        results = modely.track(frame, persist=True, retina_masks=True, boxes=True, show_conf=False, line_width=1,  conf=0.6, iou=0.5,  classes=0, show_labels=False, device=device,verbose = False,tracker=\"bytetrack.yaml\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if results[0].boxes.id is not None:\n",
    "            \n",
    "            #extracting keypoints\n",
    "            body_kp = extract_body_keypoints(results = results,threshold_class=0.4,threshold_keypoint=0.4)\n",
    "            hands_kp = extract_hands_keypoints(results = results,threshold_class=0.4,threshold_keypoint=0.4)\n",
    "            head_kp = extract_head_keypoints(results = results,threshold_class=0.4,threshold_keypoint=0.4)\n",
    "            #calculating distances between keypoints\n",
    "\n",
    "            dd = calc_distances(hands_kp,body_kp,head_kp)\n",
    "            #appending distances dictionary and evaluating average distance and classification based on it\n",
    "            for key in dd.keys():\n",
    "\n",
    "                if key not in distance_dict.keys():\n",
    "                    distance_dict[key] = deque(maxlen=40)\n",
    "\n",
    "                distance_dict[key].append(dd[key])\n",
    "                \n",
    "                if len(distance_dict[key]) == winsize:\n",
    "                    nums_sequences = nums_sequences + 1\n",
    "                    keypoints = preprocess_keypoints(distance_dict[key])\n",
    "                    logits = conv_net(keypoints.to(device,dtype = torch.float))\n",
    "                    prediction = int(torch.argmax(logits, dim=1).cpu())\n",
    "                    print(logits)\n",
    "                    ans = label_map[prediction]\n",
    "                    distance_dict[key].clear()\n",
    "                    if ans == 'fighting':\n",
    "                        break\n",
    "                    \n",
    "\n",
    "            text_size, _ = cv2.getTextSize(ans, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)\n",
    "            text_position = (frame_width - text_size[0] - 10, text_size[1] + 10)\n",
    "            cv2.rectangle(frame, (text_position[0] - 5, text_position[1] - text_size[1] - 5),\n",
    "                                    (text_position[0] + text_size[0] + 5, text_position[1] + 5), color=(0, 0, 0),\n",
    "                                    thickness=cv2.FILLED)\n",
    "            cv2.putText(frame, ans, text_position, cv2.FONT_HERSHEY_SIMPLEX, font_scale, color_map[ans], thickness, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "                    \n",
    "            \n",
    "\n",
    "        annotated_frame_show = cv2.resize(frame, (1080, 720))\n",
    "        out.write(frame)\n",
    "        cv2.imshow(\"YOLOv8 Inference\", annotated_frame_show)\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
