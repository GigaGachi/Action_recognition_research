{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing dependecies and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_path = r'D:\\Datasets\\fight'\n",
    "actions = ['fighting','not_fighting']\n",
    "fighting_path = r'D:\\Datasets\\fight\\fighting'\n",
    "not_fighting_path = r'D:\\Datasets\\fight\\not_fighting'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main functions to process and extract keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_distances(hands_dict,body_kp,head_kp):\n",
    "\n",
    "    # creating a dictionary of distances between each keypoint (except of the same object) in the keypoint_dict\n",
    "    dist_dict = {}\n",
    "    keyhead = head_kp.keys()\n",
    "    keysh = hands_dict.keys()\n",
    "    keysb = body_kp.keys()\n",
    "\n",
    "    # calculating distances between keypoints on each hand: left to left, right to left, left to right and right to right\n",
    "    for i,keyi in enumerate(keysh,start =1):\n",
    "        for j,keyj in enumerate(keysh,start =1):\n",
    "            if j>=i:\n",
    "                break\n",
    "            distll = calc_euclid_dist(hands_dict[keyi][0],hands_dict[keyj][0])\n",
    "            distlr = calc_euclid_dist(hands_dict[keyi][0],hands_dict[keyj][1])\n",
    "            distrl = calc_euclid_dist(hands_dict[keyi][1],hands_dict[keyj][0])\n",
    "            distrr = calc_euclid_dist(hands_dict[keyi][1],hands_dict[keyj][1])\n",
    "            dist_dict[f'{keyi}'+f'{keyj}'] = list([distll,distlr,distrl,distrr])\n",
    " \n",
    "    # calculating distances between hands and bodies\n",
    "    for i,keyi in enumerate(keysh,start =1):\n",
    "        for j,keyj in enumerate(keysb,start =1):\n",
    "            if j>=i:\n",
    "                break\n",
    "\n",
    "            distlb = calc_euclid_dist(hands_dict[keyi][0],body_kp[keyj])\n",
    "            \n",
    "            distrb = calc_euclid_dist(body_kp[keyj],hands_dict[keyi][1])\n",
    "\n",
    "            dist_dict[f'{keyi}'+f'{keyj}'].append(distlb)\n",
    "            dist_dict[f'{keyi}'+f'{keyj}'].append(distrb)\n",
    "                \n",
    "    for i,keyi in enumerate(keysb,start =1):\n",
    "        for j,keyj in enumerate(keysh,start =1):\n",
    "            if j>=i:\n",
    "                break\n",
    "\n",
    "            distlb = calc_euclid_dist(hands_dict[keyj][0],body_kp[keyi])\n",
    "            \n",
    "            distrb = calc_euclid_dist(body_kp[keyi],hands_dict[keyj][1])\n",
    "            \n",
    "\n",
    "            dist_dict[f'{keyi}'+f'{keyj}'].append(distlb)\n",
    "            dist_dict[f'{keyi}'+f'{keyj}'].append(distrb)\n",
    "\n",
    "\n",
    "    # calculating distances between hands and heads\n",
    "    for i,keyi in enumerate(keysh,start =1):\n",
    "        for j,keyj in enumerate(keyhead,start =1):\n",
    "            if j>=i:\n",
    "                break\n",
    "\n",
    "            distlh = calc_euclid_dist(hands_dict[keyi][0],head_kp[keyj])\n",
    "            \n",
    "            distrh = calc_euclid_dist(head_kp[keyj],hands_dict[keyi][1])\n",
    "            \n",
    "            dist_dict[f'{keyi}'+f'{keyj}'].append(distlh)\n",
    "            dist_dict[f'{keyi}'+f'{keyj}'].append(distrh)\n",
    "\n",
    "\n",
    "\n",
    "    for i,keyi in enumerate(keyhead,start =1):\n",
    "        for j,keyj in enumerate(keysh,start =1):\n",
    "            if j>=i:\n",
    "                break\n",
    "\n",
    "            distlh = calc_euclid_dist(hands_dict[keyj][0],head_kp[keyi])\n",
    "            \n",
    "            distrh = calc_euclid_dist(head_kp[keyi],hands_dict[keyj][1])\n",
    "            \n",
    " \n",
    "            dist_dict[f'{keyi}'+f'{keyj}'].append(distlh)\n",
    "            dist_dict[f'{keyi}'+f'{keyj}'].append(distrh)\n",
    "\n",
    "\n",
    "\n",
    "    # calculating distances between bodies\n",
    "    for i,keyi in enumerate(keysb,start =1):\n",
    "        for j,keyj in enumerate(keysb,start =1):\n",
    "            if j>=i:\n",
    "                break\n",
    "\n",
    "            distbb = calc_euclid_dist(body_kp[keyi],body_kp[keyj])\n",
    "\n",
    "            dist_dict[f'{keyi}'+f'{keyj}'].append(distbb)\n",
    "\n",
    "\n",
    "    # calculating distances between heads\n",
    "    for i,keyi in enumerate(keyhead,start =1):\n",
    "        for j,keyj in enumerate(keyhead,start =1):\n",
    "            if j>=i:\n",
    "                break\n",
    "\n",
    "            disthh = calc_euclid_dist(head_kp[keyi],head_kp[keyj])\n",
    "\n",
    "            dist_dict[f'{keyi}'+f'{keyj}'].append(disthh)\n",
    "    \n",
    "    return dist_dict\n",
    "\n",
    "\n",
    "def extract_hands_keypoints(results, threshold_class, threshold_keypoint):\n",
    "    # creating a dictionary to collect keypoints to each object id as dictionary key\n",
    "    existing_kp = {}\n",
    "    for result,i_d in zip(results[0],results[0].boxes.id):\n",
    "        # There results for bounding boxes, and confidence scores for general detect\n",
    "        x1, y1, x2, y2,_, conf_for_detect, class_id_detected = (result.boxes.data.tolist())[0]\n",
    "        # If the confidence score for general detect is lower than threshold, skip\n",
    "        if conf_for_detect < threshold_class:\n",
    "            continue\n",
    "        # keypoints\n",
    "        keys = (result.keypoints.data.tolist())[0]\n",
    "        xl_key, yl_key, confl = keys[9]\n",
    "        if confl > threshold_keypoint:\n",
    "           l = [int(xl_key),int(yl_key)]\n",
    "        else:\n",
    "            l = []\n",
    "        xr_key, yr_key, confr = keys[10]\n",
    "        if confr > threshold_keypoint:\n",
    "           r = [int(xr_key),int(yr_key)]\n",
    "        else:\n",
    "            r = []\n",
    "        hands_coords = list([l,r])\n",
    "        # Adding existing hand keypoints of an object in a frame to the dictionary   \n",
    "        existing_kp[int(i_d)] = hands_coords\n",
    "    return existing_kp\n",
    "\n",
    "def extract_body_keypoints(results,threshold_class, threshold_keypoint):\n",
    "    # creating a dictionary to collect keypoints to each object id as dictionary key\n",
    "    existing_kp = {}\n",
    "    for result,i_d in zip(results[0],results[0].boxes.id):\n",
    "        # There results for bounding boxes, and confidence scores for general detect\n",
    "        x1, y1, x2, y2,_, conf_for_detect, class_id_detected = (result.boxes.data.tolist())[0]\n",
    "        # If the confidence score for general detect is lower than threshold, skip\n",
    "        if conf_for_detect < threshold_class:\n",
    "            continue\n",
    "        # keypoints\n",
    "        keys = (result.keypoints.data.tolist())[0]\n",
    "        xl_key, yl_key, confl = keys[5]\n",
    "        xr_key, yr_key, confr = keys[6]\n",
    "        if (confl>threshold_keypoint) and (confr>threshold_keypoint):\n",
    "            # Adding existing hand keypoints of an object in a frame to the dictionary   \n",
    "            mid_point  = list([int((xr_key+xl_key)/2),int((yl_key+yr_key)/2)])\n",
    "            \n",
    "        else:\n",
    "            mid_point = []\n",
    "\n",
    "        existing_kp[int(i_d)] = mid_point\n",
    "\n",
    "    return existing_kp\n",
    "\n",
    "def extract_head_keypoints(results,threshold_class, threshold_keypoint):\n",
    "    # creating a dictionary to collect keypoints to each object id as dictionary key\n",
    "    existing_kp = {}\n",
    "    for result,i_d in zip(results[0],results[0].boxes.id):\n",
    "        # There results for bounding boxes, and confidence scores for general detect\n",
    "        x1, y1, x2, y2,_, conf_for_detect, class_id_detected = (result.boxes.data.tolist())[0]\n",
    "        # If the confidence score for general detect is lower than threshold, skip\n",
    "        if conf_for_detect < threshold_class:\n",
    "            continue\n",
    "        # keypoints\n",
    "        keys = (result.keypoints.data.tolist())[0]\n",
    "        xh_key, yh_key, confh = keys[0]\n",
    "        if confh>threshold_keypoint:\n",
    "            # Adding existing hand keypoints of an object in a frame to the dictionary   \n",
    "            mid_point  = list([int(xh_key),int(yh_key)])\n",
    "        else:\n",
    "            mid_point = []\n",
    "        existing_kp[int(i_d)] = mid_point\n",
    "    return existing_kp\n",
    "\n",
    "\n",
    "def extract_keypoints(results, threshold_class):\n",
    "    existing_kp = {}\n",
    "    for result,i_d in zip(results[0],results[0].boxes.id):\n",
    "        # There results for bounding boxes, and confidence scores for general detect\n",
    "        x1, y1, x2, y2,_, conf_for_detect, class_id_detected = (result.boxes.data.tolist())[0]\n",
    "        # If the confidence score for general detect is lower than threshold, skip\n",
    "        if conf_for_detect < threshold_class:\n",
    "            continue\n",
    "        # keypoints\n",
    "        keys = (result.keypoints.data.tolist())[0]\n",
    "        keyp_arr = list()\n",
    "        for key in keys:\n",
    "            keyp_arr.append(key)\n",
    "        # Adding existing hand keypoints of an object in a frame to the dictionary   \n",
    "        existing_kp[int(i_d)] = keyp_arr\n",
    "    return existing_kp\n",
    "\n",
    "def calc_kp_to_kp_dist(keypoints_dict):\n",
    "    # creating a dictionary of distances between each keypoint (except of the same object) in the keypoint_dict\n",
    "    dist_dict = {}\n",
    "    keys = keypoints_dict.keys()\n",
    "    # calculating distances between keypoints \n",
    "    for l,keyi in enumerate(keys,start =1):\n",
    "        for m,keyj in enumerate(keys,start =1):\n",
    "            if m>=l:\n",
    "                break  \n",
    "            for i,p1 in enumerate(keypoints_dict[keyi]):\n",
    "                for j,p2 in enumerate(keypoints_dict[keyj]):\n",
    "                    dist = calc_euclid_dist(p1,p2)\n",
    "                    dist_dict[f'{keyi}'+f'{keyj}'+f'{i}'+f'{j}'] = dist\n",
    "    return dist_dict\n",
    "\n",
    "def calc_euclid_dist(p1,p2):\n",
    "    if (len(p1)>0) and (len(p2)>0):\n",
    "        dist = int(math.sqrt((p1[0]-p2[0])*(p1[0]-p2[0]) + (p1[1]-p2[1])*(p1[1]-p2[1])))\n",
    "        return dist\n",
    "    else: \n",
    "        return np.nan\n",
    "    \n",
    "def calc_grad(dist_dict):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing dictionaries and parametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"No suspicious activity\"\n",
    "text1 = \"Suspicious activity\"\n",
    "text3 = \"No people in sight\"\n",
    "color2 = (100, 200, 0)\n",
    "color1 = (100, 0, 200)\n",
    "color3 = (100, 100, 100)\n",
    "font_scale = 1.6\n",
    "thickness = 2\n",
    "\n",
    "winsize = 40\n",
    "all_keypoints = {}\n",
    "distance_dict = {}\n",
    "average_dist = {}\n",
    "grad_dict = {}\n",
    "outputs = [0,1]\n",
    "nums_sequences = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing YOLOv8 pose model and caption from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "modely = YOLO('yolov8l-pose.pt')  # load a pretrained YOLOv8n classification model\n",
    "modely.to(device)\n",
    "video_path = r\"D:\\videos\\fight4.mp4\"\n",
    "vid_name = 'v4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "# Get video properties\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) # or number\n",
    "# Create a VideoWriter object to save the output video\n",
    "output_video_path = r\"D:\\videos_processed\\fight4_processed.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data coleccting cycle. Each (winsize) frames you will have to press 'f' key if fight was on those frames and any other key if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pair 42.\n",
      "Processing pair 41.\n",
      "Processing pair 12.\n",
      "Processing pair 92.\n",
      "Processing pair 21.\n",
      "Processing pair 12.\n",
      "Processing pair 12.\n",
      "Processing pair 228.\n",
      "Processing pair 3528.\n",
      "Processing pair 3628.\n",
      "Processing pair 3928.\n",
      "Processing pair 3936.\n",
      "Processing pair 3628.\n",
      "Processing pair 3628.\n",
      "Processing pair 3628.\n",
      "Processing pair 4436.\n",
      "Processing pair 4536.\n",
      "Processing pair 2836.\n",
      "Processing pair 4636.\n",
      "Processing pair 4836.\n",
      "Processing pair 4936.\n",
      "Processing pair 4948.\n",
      "Processing pair 4836.\n",
      "Processing pair 4936.\n",
      "Processing pair 4948.\n",
      "Processing pair 5036.\n",
      "Processing pair 5048.\n",
      "Processing pair 5049.\n",
      "Processing pair 4836.\n",
      "Processing pair 4936.\n",
      "Processing pair 4948.\n",
      "Processing pair 5036.\n",
      "Processing pair 5048.\n",
      "Processing pair 5049.\n",
      "Processing pair 4836.\n",
      "Processing pair 4936.\n",
      "Processing pair 4948.\n",
      "Processing pair 4836.\n",
      "Processing pair 5036.\n",
      "Processing pair 5048.\n",
      "Processing pair 4836.\n",
      "Processing pair 4836.\n",
      "Processing pair 5848.\n",
      "Processing pair 5948.\n",
      "Processing pair 5958.\n",
      "Processing pair 5948.\n",
      "Processing pair 6148.\n",
      "Processing pair 6159.\n",
      "Processing pair 5948.\n",
      "Processing pair 6148.\n",
      "Processing pair 6361.\n",
      "Processing pair 4861.\n",
      "Processing pair 4863.\n",
      "Processing pair 6461.\n",
      "Processing pair 6463.\n",
      "Processing pair 4864.\n",
      "Processing pair 6361.\n",
      "Processing pair 4861.\n",
      "Processing pair 4863.\n",
      "Processing pair 6861.\n",
      "Processing pair 7161.\n",
      "Processing pair 7176.\n",
      "Processing pair 6176.\n",
      "Processing pair 7161.\n",
      "Processing pair 7661.\n",
      "Processing pair 8061.\n",
      "Processing pair 8161.\n",
      "Processing pair 8180.\n",
      "Processing pair 7681.\n",
      "Processing pair 7661.\n",
      "Processing pair 8681.\n",
      "Processing pair 8781.\n",
      "Processing pair 8786.\n",
      "Processing pair 8681.\n",
      "Processing pair 8781.\n",
      "Processing pair 8786.\n",
      "Processing pair 8681.\n",
      "Processing pair 9086.\n",
      "Processing pair 9286.\n",
      "Processing pair 9290.\n",
      "Processing pair 8186.\n",
      "Processing pair 8190.\n",
      "Processing pair 8192.\n",
      "Processing pair 9086.\n",
      "Processing pair 9286.\n",
      "Processing pair 8186.\n",
      "Processing pair 8192.\n",
      "Processing pair 8186.\n",
      "Processing pair 8695.\n",
      "Processing pair 10086.\n",
      "Processing pair 10486.\n",
      "Processing pair 104100.\n",
      "Processing pair 10086.\n",
      "Processing pair 10486.\n",
      "Processing pair 104100.\n",
      "Processing pair 10086.\n",
      "Processing pair 104100.\n",
      "Processing pair 86100.\n",
      "Processing pair 10486.\n",
      "Processing pair 104100.\n",
      "Processing pair 107100.\n",
      "Processing pair 10786.\n",
      "Processing pair 86100.\n",
      "Processing pair 104107.\n",
      "Processing pair 10486.\n",
      "Processing pair 104100.\n",
      "Processing pair 86100.\n",
      "Processing pair 11286.\n",
      "Processing pair 114112.\n",
      "Processing pair 117112.\n",
      "Processing pair 117114.\n",
      "Processing pair 86114.\n",
      "Processing pair 120119.\n",
      "Processing pair 122121.\n",
      "Processing pair 124121.\n",
      "Processing pair 124122.\n",
      "Processing pair 122121.\n",
      "Processing pair 124121.\n",
      "Processing pair 124122.\n",
      "Processing pair 130121.\n",
      "Processing pair 130122.\n",
      "Processing pair 130124.\n",
      "Processing pair 133121.\n",
      "Processing pair 133122.\n",
      "Processing pair 133124.\n",
      "Processing pair 133130.\n",
      "Processing pair 124122.\n",
      "Processing pair 122124.\n",
      "Processing pair 122124.\n",
      "Processing pair 124148.\n",
      "Processing pair 146124.\n",
      "Processing pair 146148.\n",
      "Processing pair 124148.\n",
      "Processing pair 151148.\n",
      "Processing pair 151124.\n",
      "Processing pair 148124.\n",
      "Processing pair 158148.\n",
      "Processing pair 161148.\n",
      "Processing pair 161158.\n",
      "Processing pair 124148.\n",
      "Processing pair 158148.\n",
      "Processing pair 124158.\n",
      "Processing pair 124161.\n",
      "Processing pair 161148.\n",
      "Processing pair 161158.\n",
      "Processing pair 165158.\n",
      "Processing pair 165161.\n",
      "Processing pair 124148.\n",
      "Processing pair 124165.\n",
      "Processing pair 124158.\n",
      "Processing pair 148124.\n",
      "Processing pair 165158.\n",
      "Processing pair 148158.\n",
      "Processing pair 148165.\n",
      "Processing pair 124165.\n",
      "Processing pair 124161.\n",
      "Processing pair 148161.\n",
      "Processing pair 124161.\n",
      "Processing pair 124148.\n",
      "Processing pair 167148.\n",
      "Processing pair 167124.\n",
      "Processing pair 124148.\n",
      "Processing pair 124148.\n",
      "Processing pair 148124.\n",
      "Processing pair 169148.\n",
      "Processing pair 173148.\n",
      "Processing pair 176148.\n",
      "Processing pair 173148.\n",
      "Processing pair 179148.\n",
      "Processing pair 179176.\n",
      "Processing pair 173179.\n",
      "Processing pair 173148.\n",
      "Processing pair 148173.\n",
      "Processing pair 185173.\n",
      "Processing pair 148185.\n",
      "Processing pair 185173.\n",
      "Processing pair 148173.\n",
      "Processing pair 148185.\n",
      "Processing pair 148173.\n",
      "Processing pair 148173.\n",
      "Processing pair 208173.\n",
      "Processing pair 208148.\n",
      "Processing pair 210173.\n",
      "Processing pair 210148.\n",
      "Processing pair 208210.\n",
      "Processing pair 148173.\n",
      "Processing pair 210148.\n",
      "Processing pair 208148.\n",
      "Processing pair 208173.\n",
      "Processing pair 217148.\n",
      "Processing pair 217210.\n",
      "Processing pair 208173.\n",
      "Processing pair 219173.\n",
      "Processing pair 224173.\n",
      "Processing pair 219173.\n",
      "Processing pair 219224.\n",
      "Processing pair 235234.\n",
      "Processing pair 233234.\n",
      "Processing pair 233235.\n",
      "Processing pair 235234.\n",
      "Processing pair 247235.\n",
      "Processing pair 247234.\n",
      "Processing pair 233235.\n",
      "Processing pair 234235.\n",
      "Processing pair 233235.\n",
      "Processing pair 233234.\n",
      "Processing pair 235233.\n",
      "Processing pair 248233.\n",
      "Processing pair 249233.\n",
      "Processing pair 249248.\n",
      "Processing pair 248233.\n",
      "Processing pair 249233.\n",
      "Processing pair 249233.\n",
      "Processing pair 257233.\n",
      "Processing pair 257249.\n",
      "Processing pair 249233.\n",
      "Processing pair 261249.\n",
      "Processing pair 261249.\n",
      "Processing pair 233249.\n",
      "Processing pair 233261.\n",
      "Processing pair 261249.\n",
      "Processing pair 233249.\n",
      "Processing pair 233261.\n",
      "Processing pair 261249.\n",
      "Processing pair 233249.\n",
      "Processing pair 233261.\n",
      "Processing pair 266249.\n",
      "Processing pair 261249.\n",
      "Processing pair 261266.\n",
      "Processing pair 266249.\n",
      "Processing pair 268249.\n",
      "Processing pair 266249.\n",
      "Processing pair 266249.\n",
      "Processing pair 269249.\n",
      "Processing pair 277249.\n",
      "Processing pair 269249.\n",
      "Processing pair 279249.\n",
      "Processing pair 279269.\n",
      "Processing pair 269249.\n",
      "Processing pair 269249.\n",
      "Processing pair 284249.\n",
      "Processing pair 284269.\n",
      "Processing pair 269249.\n",
      "Processing pair 269249.\n",
      "Processing pair 294249.\n",
      "Processing pair 294269.\n",
      "Processing pair 269249.\n",
      "Processing pair 294249.\n",
      "Processing pair 294269.\n",
      "Processing pair 269249.\n",
      "Processing pair 269249.\n",
      "Processing pair 305249.\n",
      "Processing pair 305269.\n",
      "Processing pair 269249.\n",
      "Processing pair 305249.\n",
      "Processing pair 305269.\n",
      "Processing pair 306249.\n",
      "Processing pair 306269.\n",
      "Processing pair 306305.\n",
      "Processing pair 269249.\n",
      "Processing pair 305249.\n",
      "Processing pair 305269.\n",
      "Processing pair 269249.\n",
      "Processing pair 249305.\n",
      "Processing pair 269305.\n",
      "Processing pair 269249.\n",
      "Processing pair 269249.\n",
      "Processing pair 269249.\n",
      "Processing pair 269249.\n",
      "Processing pair 317249.\n",
      "Processing pair 318316.\n",
      "Processing pair 320316.\n",
      "Processing pair 323316.\n",
      "Processing pair 331316.\n",
      "Processing pair 323316.\n",
      "Processing pair 323316.\n",
      "Processing pair 343316.\n",
      "Processing pair 323343.\n",
      "Processing pair 323316.\n",
      "Processing pair 343316.\n",
      "Processing pair 323343.\n",
      "Processing pair 323316.\n",
      "Processing pair 343316.\n",
      "Processing pair 323343.\n",
      "Processing pair 365316.\n",
      "Processing pair 365343.\n",
      "Processing pair 365323.\n",
      "Processing pair 323316.\n",
      "Processing pair 343316.\n",
      "Processing pair 323343.\n",
      "Processing pair 365343.\n",
      "Processing pair 365323.\n",
      "Processing pair 316323.\n",
      "Processing pair 343323.\n",
      "Processing pair 370323.\n",
      "Processing pair 370316.\n",
      "Processing pair 365323.\n",
      "Processing pair 365343.\n",
      "Processing pair 343316.\n",
      "Processing pair 316323.\n",
      "Processing pair 343316.\n",
      "Processing pair 323343.\n",
      "Processing pair 323316.\n",
      "Processing pair 343323.\n",
      "Processing pair 323343.\n",
      "Processing pair 379323.\n",
      "Processing pair 316343.\n",
      "Processing pair 379343.\n",
      "Processing pair 316323.\n",
      "Processing pair 343316.\n",
      "Processing pair 316323.\n",
      "Processing pair 343323.\n",
      "Processing pair 388323.\n",
      "Processing pair 388343.\n",
      "Processing pair 343323.\n",
      "Processing pair 392323.\n",
      "Processing pair 392343.\n",
      "Processing pair 388323.\n",
      "Processing pair 343323.\n",
      "Processing pair 323343.\n",
      "Processing pair 417415.\n",
      "Processing pair 343415.\n",
      "Processing pair 343417.\n",
      "Processing pair 417415.\n",
      "Processing pair 343415.\n",
      "Processing pair 343417.\n",
      "Processing pair 417415.\n",
      "Processing pair 415430.\n",
      "Processing pair 438430.\n",
      "Processing pair 438415.\n",
      "Processing pair 415430.\n",
      "Processing pair 430415.\n",
      "Processing pair 438415.\n",
      "Processing pair 430438.\n",
      "Processing pair 415430.\n",
      "Processing pair 415438.\n",
      "Processing pair 460430.\n",
      "Processing pair 415460.\n",
      "Processing pair 415430.\n",
      "Processing pair 460430.\n",
      "Processing pair 415460.\n",
      "Processing pair 415430.\n",
      "Processing pair 460430.\n",
      "Processing pair 415460.\n",
      "Processing pair 415430.\n",
      "Processing pair 463430.\n",
      "Processing pair 463460.\n",
      "Processing pair 463415.\n",
      "Processing pair 430415.\n",
      "Processing pair 474477.\n",
      "Processing pair 497477.\n",
      "Processing pair 490477.\n",
      "Processing pair 502477.\n",
      "Processing pair 490477.\n",
      "Processing pair 503477.\n",
      "Processing pair 503477.\n",
      "Processing pair 512477.\n",
      "Processing pair 512503.\n",
      "Processing pair 503477.\n",
      "Processing pair 512477.\n",
      "Processing pair 512503.\n",
      "Processing pair 514503.\n",
      "Processing pair 514512.\n",
      "Processing pair 477512.\n",
      "Processing pair 477514.\n",
      "Processing pair 477503.\n",
      "Processing pair 503512.\n",
      "Processing pair 477512.\n",
      "Processing pair 477503.\n",
      "Processing pair 503512.\n",
      "Processing pair 520512.\n",
      "Processing pair 520503.\n",
      "Processing pair 520477.\n",
      "Processing pair 520477.\n",
      "Processing pair 523477.\n",
      "Processing pair 525523.\n",
      "Processing pair 477523.\n",
      "Processing pair 525477.\n",
      "Processing pair 525477.\n",
      "Processing pair 539477.\n",
      "Processing pair 539525.\n",
      "Processing pair 477525.\n",
      "Processing pair 525551.\n",
      "Processing pair 553551.\n",
      "Processing pair 525553.\n",
      "Processing pair 553551.\n",
      "Processing pair 525551.\n",
      "Processing pair 525553.\n",
      "Processing pair 553551.\n",
      "Processing pair 525551.\n",
      "Processing pair 566561.\n",
      "Processing pair 565561.\n",
      "Processing pair 566561.\n",
      "Processing pair 565566.\n",
      "Processing pair 576561.\n",
      "Processing pair 577561.\n",
      "Processing pair 586585.\n",
      "Processing pair 585592.\n",
      "Processing pair 585592.\n",
      "Processing pair 592585.\n",
      "Processing pair 592585.\n",
      "Processing pair 603585.\n",
      "Processing pair 592585.\n",
      "Processing pair 608585.\n",
      "Processing pair 608592.\n",
      "Processing pair 592585.\n",
      "Processing pair 620585.\n",
      "Processing pair 585620.\n",
      "Processing pair 625620.\n",
      "Processing pair 625585.\n",
      "Processing pair 626620.\n",
      "Processing pair 585620.\n",
      "Processing pair 626585.\n",
      "Processing pair 585620.\n",
      "Processing pair 620585.\n",
      "Processing pair 585647.\n",
      "Processing pair 585647.\n",
      "Processing pair 651647.\n",
      "Processing pair 585647.\n",
      "Processing pair 651585.\n",
      "Processing pair 651647.\n",
      "Processing pair 647585.\n",
      "Processing pair 651585.\n",
      "Processing pair 651647.\n",
      "Processing pair 647585.\n",
      "Processing pair 651585.\n",
      "Processing pair 651647.\n",
      "Processing pair 647585.\n",
      "Processing pair 585647.\n",
      "Processing pair 585647.\n",
      "Processing pair 585647.\n",
      "Processing pair 677683.\n",
      "Processing pair 677688.\n",
      "Processing pair 683677.\n",
      "Processing pair 683677.\n",
      "Processing pair 683677.\n",
      "Processing pair 694677.\n",
      "Processing pair 694683.\n",
      "Processing pair 683677.\n",
      "Processing pair 694677.\n",
      "Processing pair 694683.\n",
      "Processing pair 683677.\n",
      "Processing pair 683677.\n",
      "Processing pair 694677.\n",
      "Processing pair 694683.\n",
      "Processing pair 683677.\n",
      "Processing pair 694677.\n",
      "Processing pair 694683.\n",
      "Processing pair 683677.\n",
      "Processing pair 694677.\n",
      "Processing pair 694683.\n",
      "Processing pair 683677.\n",
      "Processing pair 694677.\n",
      "Processing pair 694683.\n",
      "Processing pair 683677.\n",
      "Processing pair 683677.\n",
      "Processing pair 704677.\n",
      "Processing pair 704683.\n",
      "Processing pair 683677.\n",
      "Processing pair 683677.\n",
      "Processing pair 708677.\n",
      "Processing pair 683708.\n",
      "Processing pair 677683.\n",
      "Processing pair 677708.\n",
      "Processing pair 683708.\n",
      "Processing pair 677683.\n",
      "Processing pair 677708.\n",
      "Processing pair 683708.\n",
      "Processing pair 677683.\n",
      "Processing pair 683677.\n",
      "Processing pair 683677.\n",
      "Processing pair 683677.\n",
      "Processing pair 683677.\n",
      "Processing pair 727683.\n",
      "Processing pair 741727.\n",
      "Processing pair 742727.\n",
      "Processing pair 743742.\n",
      "Processing pair 727742.\n",
      "Processing pair 727744.\n",
      "Processing pair 744742.\n",
      "Processing pair 727742.\n",
      "Processing pair 744727.\n",
      "Processing pair 748742.\n",
      "Processing pair 748727.\n",
      "Processing pair 727742.\n",
      "Processing pair 752727.\n",
      "Processing pair 742727.\n",
      "Processing pair 742752.\n",
      "Processing pair 753727.\n",
      "Processing pair 753752.\n",
      "Processing pair 753742.\n",
      "Processing pair 752727.\n",
      "Processing pair 742727.\n",
      "Processing pair 753742.\n",
      "Processing pair 727742.\n",
      "Processing pair 727753.\n",
      "Processing pair 752742.\n",
      "Processing pair 752753.\n",
      "Processing pair 727752.\n",
      "Processing pair 755742.\n",
      "Processing pair 755753.\n",
      "Processing pair 755752.\n",
      "Processing pair 755727.\n",
      "Processing pair 753742.\n",
      "Processing pair 727742.\n",
      "Processing pair 727753.\n",
      "Processing pair 752742.\n",
      "Processing pair 752753.\n",
      "Processing pair 758742.\n",
      "Processing pair 758753.\n",
      "Processing pair 758727.\n",
      "Processing pair 752727.\n",
      "Processing pair 753742.\n",
      "Processing pair 727742.\n",
      "Processing pair 727753.\n",
      "Processing pair 765753.\n",
      "Processing pair 758753.\n",
      "Processing pair 758742.\n",
      "Processing pair 742753.\n",
      "Processing pair 742765.\n",
      "Processing pair 758765.\n",
      "Processing pair 742773.\n",
      "Processing pair 778773.\n",
      "Processing pair 785778.\n",
      "Processing pair 787778.\n",
      "Processing pair 785787.\n",
      "Processing pair 785778.\n",
      "Processing pair 787778.\n",
      "Processing pair 785787.\n",
      "Processing pair 794778.\n",
      "Processing pair 794787.\n",
      "Processing pair 785794.\n",
      "Processing pair 785778.\n",
      "Processing pair 787778.\n",
      "Processing pair 785778.\n",
      "Processing pair 787785.\n",
      "Processing pair 812778.\n",
      "Processing pair 814778.\n",
      "Processing pair 812814.\n",
      "Processing pair 813778.\n",
      "Processing pair 813814.\n",
      "Processing pair 812778.\n",
      "Processing pair 814778.\n",
      "Processing pair 812813.\n",
      "Processing pair 812778.\n",
      "Processing pair 814812.\n",
      "Processing pair 814778.\n",
      "Processing pair 814812.\n",
      "Processing pair 816812.\n",
      "Processing pair 816778.\n",
      "Processing pair 814778.\n",
      "Processing pair 816814.\n",
      "Processing pair 778812.\n",
      "Processing pair 814812.\n",
      "Processing pair 816812.\n",
      "Processing pair 814778.\n",
      "Processing pair 814778.\n",
      "Processing pair 814778.\n",
      "Processing pair 827814.\n",
      "Processing pair 831814.\n",
      "Processing pair 827814.\n",
      "Processing pair 827831.\n",
      "Processing pair 831814.\n",
      "Processing pair 827814.\n",
      "Processing pair 837814.\n",
      "Processing pair 837827.\n",
      "Processing pair 827814.\n"
     ]
    }
   ],
   "source": [
    "while cap.isOpened():\n",
    "# Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "\n",
    "        results = modely.track(frame, persist=True, retina_masks=True, boxes=True, show_conf=False, line_width=1,  conf=0.8, iou=0.5,  classes=0, show_labels=False, device=device,verbose = False,tracker=\"bytetrack.yaml\")\n",
    "        if results[0].boxes.id is not None:\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)\n",
    "            ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
    "            for box, i_d in zip(boxes, ids):\n",
    "                x1, y1, x2, y2 = box[0], box[1], box[2], box[3]\n",
    "\n",
    "\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw customer id on the frame above the bounding box\n",
    "                text = f\"{i_d}\"\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                font_scale = 0.6\n",
    "                font_thickness = 1\n",
    "                text_size = cv2.getTextSize(text, font, font_scale, font_thickness)[0]\n",
    "\n",
    "                # Calculate the position to align the label with the top of the bounding box\n",
    "                text_x = x1 + (x2 - x1 - text_size[0]) // 2\n",
    "                text_y = y1 - 10  # Adjust this value for the desired vertical offset\n",
    "\n",
    "                # Make sure the text_y position is within the frame's bounds\n",
    "                if text_y < 0:\n",
    "                    text_y = 0\n",
    "\n",
    "                # Draw the label background rectangle\n",
    "                cv2.rectangle(frame, (text_x - 5, text_y - text_size[1] - 5), (text_x + text_size[0] + 5, text_y + 5), (0, 0, 0), -1)\n",
    "\n",
    "                #Draw the customer id text\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    text,\n",
    "                    (text_x, text_y),\n",
    "                    font,\n",
    "                    font_scale,\n",
    "                    (255, 255, 255),  # White color\n",
    "                    font_thickness,\n",
    "                    lineType=cv2.LINE_AA\n",
    "                )\n",
    "    \n",
    "            #extracting keypoints\n",
    "            body_kp = extract_body_keypoints(results = results,threshold_class=0.4,threshold_keypoint=0.4)\n",
    "            hands_kp = extract_hands_keypoints(results = results,threshold_class=0.4,threshold_keypoint=0.4)\n",
    "            head_kp = extract_head_keypoints(results = results,threshold_class=0.4,threshold_keypoint=0.4)\n",
    "            #calculating distances between keypoints\n",
    "\n",
    "            dd = calc_distances(hands_kp,body_kp,head_kp)\n",
    "            #appending distances dictionary and evaluating average distance and classification based on it\n",
    "            for key in dd.keys():\n",
    "\n",
    "                if key not in distance_dict.keys():\n",
    "                    distance_dict[key] = deque(maxlen=40)\n",
    "\n",
    "                distance_dict[key].append(dd[key])\n",
    "                \n",
    "                if len(distance_dict[key]) == winsize:\n",
    "                    nums_sequences = nums_sequences + 1\n",
    "                    print(f'Processing pair {key}.')\n",
    "                    keypoints = np.array(distance_dict[key])\n",
    "                    if cv2.waitKey(-1) & 0xFF == ord('f'):\n",
    "                        if cv2.waitKey(-1) & 0xFF == ord('f'):\n",
    "                            save_path = fighting_path   + f'\\{vid_name}' +  f'{nums_sequences}'\n",
    "                        else:\n",
    "                            save_path = not_fighting_path  + f'\\{vid_name}' +  f'{nums_sequences}'\n",
    "                    else:\n",
    "                        distance_dict[key].clear()\n",
    "                        continue\n",
    "                    np.save(save_path,keypoints)\n",
    "                    distance_dict[key].clear()\n",
    "            \n",
    "\n",
    "        annotated_frame_show = cv2.resize(frame, (1080, 720))\n",
    "        cv2.imshow(\"YOLOv8 Inference\", annotated_frame_show)\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import torch\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sequence(seq):\n",
    "    mean = torch.mean(seq)\n",
    "    std_dev = torch.std(seq) + 0.0001\n",
    "    standardized_data = (seq - mean) / std_dev\n",
    "    return standardized_data\n",
    "\n",
    "#data = torch.Tensor([ np.nan,  np.nan,  np.nan,  np.nan,  np.nan,  np.nan,  np.nan,  np.nan,  np.nan,  \n",
    "  #                   np.nan])\n",
    "def approximate_linear(series):\n",
    "    i_0 = 0\n",
    "    length = series.shape[0]\n",
    "    left_value = 0\n",
    "    right_value = 0\n",
    "    for i,num in enumerate(series):\n",
    "        if torch.isnan(num) == True:\n",
    "            i_0 = i\n",
    "            for j in range(i_0,length):\n",
    "                if torch.isnan(series[j]) == False:\n",
    "                    right_value  = series[j]\n",
    "                    if (i_0) == 0:\n",
    "                        left_value = right_value \n",
    "                    else:\n",
    "                        left_value = series[i_0 - 1]\n",
    "                    \n",
    "                    tg = (right_value - left_value)/(j - i_0 + 1)\n",
    "                    for k,ind  in enumerate(range(i_0,j)):\n",
    "                        series[ind] = left_value + (k+1)*tg\n",
    "                    break\n",
    "                    \n",
    "                else:\n",
    "                    if ((j+1) == length)&(i_0 == 0):\n",
    "                        series = torch.nan_to_num(series,0)\n",
    "                        return series\n",
    "                    if (j+1) == length:\n",
    "                        for k,ind  in enumerate(range(i_0,j+1)):\n",
    "                            series[ind] = left_value\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return normalize_sequence(series)\n",
    "\n",
    "\n",
    "def approx_gaps(sequences):\n",
    "    seq_data = torch.transpose(torch.Tensor(sequences),1,2)\n",
    "    data_shape = seq_data.shape\n",
    "    output_data = torch.zeros((data_shape)) \n",
    "    for i,seq in enumerate(seq_data):\n",
    "        for j,series in enumerate(seq):\n",
    "            output_data[i][j] = approximate_linear(series)\n",
    "\n",
    "    return output_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([494, 14, 40])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.7976, 0.2024])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {label: num for num, label in enumerate(actions)}\n",
    "sequences,labels = [],[]\n",
    "for action in actions:\n",
    "    for file in os.listdir(os.path.join(DATA_path,action)):\n",
    "        sequences.append(np.load(os.path.join(DATA_path,action,file),allow_pickle=True))\n",
    "        labels.append(label_map[action])\n",
    "\n",
    "seq_data = approx_gaps(sequences=np.array(sequences))\n",
    "\n",
    "\n",
    "seq_labels = to_categorical(labels).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(seq_data,seq_labels,test_size=0.1) \n",
    "\n",
    "\n",
    "num0class = np.sum(np.argmax(y_train,axis = 1) ==0 )\n",
    "num1class = np.sum(np.argmax(y_train,axis = 1) ==1 )\n",
    "\n",
    "num01class = len(y_train)\n",
    "print(X_train.shape)\n",
    "loss_weights = torch.Tensor([1 - num0class/num01class,1 - num1class/num01class])\n",
    "loss_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfering data to pytorch compatible type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keypoint_sequence_dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx, :], self.y[idx]\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building neural network and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import wandb\n",
    "from filelock import FileLock\n",
    "from ray import train, tune\n",
    "from ray.train import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "#wandb.login(key = '66d1d2eaf7cd3f83b644fc151071bbf5d7f0c237')\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq_conv_net(nn.Module):\n",
    "    def __init__(self,kernel_size = 3,dropout_conv = 0.3,dropout_linear = 0.3,attention_size = 8,attention_heads = 4,attention_dropout = 0.3):\n",
    "        super(Seq_conv_net,self).__init__()\n",
    "        \n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=14, out_channels=32, kernel_size =kernel_size,padding= 'same') ,\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout1d(dropout_conv)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size = kernel_size,padding= 'same') ,\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout1d(dropout_conv)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size = kernel_size,padding= 'same') ,\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout1d(dropout_conv)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128, out_channels=attention_size, kernel_size = kernel_size,padding= 'same') ,\n",
    "            nn.BatchNorm1d(attention_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout1d(dropout_conv)\n",
    "        )\n",
    "        self.maxpool1 = nn.AdaptiveMaxPool1d(20)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "       #self.lstm1 = nn.LSTM(64,64,LSTM_size,batch_first=True,dropout= LSTM_dropout)\n",
    "        self.attention = nn.MultiheadAttention(attention_size,attention_heads,batch_first=True,dropout= attention_dropout)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(attention_size*20, attention_size*5) ,\n",
    "            nn.BatchNorm1d(attention_size*5),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout1d(dropout_linear)\n",
    "        )\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.Linear(attention_size*5, attention_size) ,\n",
    "            nn.BatchNorm1d(attention_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout1d(dropout_linear)\n",
    "        )\n",
    "        self.linear3 = nn.Sequential(\n",
    "            nn.Linear(attention_size, 128) ,\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout1d(dropout_linear)\n",
    "        )\n",
    "        self.logits = nn.Sequential(\n",
    "            nn.Linear(128, 2) ,\n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # forward pass сети\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = torch.transpose(x,1,2)\n",
    "        #x,_ = self.lstm1(x)\n",
    "        x,_ = self.attention(x,x,x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.logits(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, loss_fn):\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    num_elements = len(dataloader.dataset)\n",
    "    num_correct = 0\n",
    "    num0_true = 0\n",
    "    num0_false = 0\n",
    "    num1_true = 0\n",
    "    num1_false = 0\n",
    "    num0_ = 0\n",
    "    num1_ = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        # так получаем текущий батч\n",
    "        X_batch, y_batch = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(X_batch.to(device,dtype = torch.float))\n",
    "            \n",
    "            loss = loss_fn(logits, y_batch.to(device,dtype = torch.float))\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            y_pred = torch.argmax(logits, dim=1).cpu()\n",
    "            \n",
    "            y_answers = torch.argmax(y_batch, dim=1).cpu()\n",
    "\n",
    "            num_correct += torch.sum(y_answers == y_pred)\n",
    "            num0_true += torch.sum((y_answers == 0)&(y_pred == 0))\n",
    "            num0_false += torch.sum((y_answers == 0)&(y_pred == 1))\n",
    "            num1_true += torch.sum((y_answers == 1)&(y_pred == 1))\n",
    "            num1_false += torch.sum((y_answers == 1)&(y_pred == 0))\n",
    "            num0_ +=torch.sum(y_answers == 0)\n",
    "            num1_ +=torch.sum(y_answers == 1)\n",
    "    accuracy = num_correct / num_elements   \n",
    "\n",
    "    f1 = F1_score(num0_true,num0_false,num1_false)\n",
    "\n",
    "    conf_matrix = np.array([[num1_true,num0_false],[num1_false,num0_true]])\n",
    "    \n",
    "    return float(accuracy), float(f1),float(np.mean(losses)), conf_matrix\n",
    "\n",
    "def F1_score(tp,fp,fn):\n",
    "    return (2*tp/(2*tp + fp + fn))\n",
    "\n",
    "def train_model(model, loss_fn, optimizer,train_loader,val_loader, n_epoch=3,raytune_mode = False,wandb_mode = False):\n",
    "\n",
    "    num_iter = 0\n",
    "    # цикл обучения сети\n",
    "    for epoch in range(n_epoch):\n",
    "\n",
    "        \n",
    "\n",
    "        model.train(True)\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            # так получаем текущий батч\n",
    "            X_batch, y_batch = batch \n",
    "            \n",
    "            # forward pass (получение ответов на батч картинок)\n",
    "            logits = model(X_batch.to(device,dtype = torch.float)) \n",
    "            \n",
    "            # вычисление лосса от выданных сетью ответов и правильных ответов на батч\n",
    "            loss = loss_fn(logits, y_batch.to(device,dtype = torch.float)) \n",
    "            \n",
    "            \n",
    "            loss.backward() # backpropagation (вычисление градиентов)\n",
    "            optimizer.step() # обновление весов сети\n",
    "            optimizer.zero_grad() # обнуляем веса\n",
    "\n",
    "            num_iter += 1\n",
    "\n",
    "\n",
    "        # после каждой эпохи получаем метрику качества на валидационной выборке\n",
    "        model.train(False)\n",
    "        \n",
    "        val_accuracy,f1_v, val_loss,_ = evaluate_model(model, val_loader, loss_fn=loss_fn)\n",
    "        train_accuracy,f1_t, train_loss,_ = evaluate_model(model, train_loader, loss_fn=loss_fn)\n",
    "        \n",
    "        if wandb_mode == True:\n",
    "            wandb.log({\"Val/accuracy\": val_accuracy,\"val/f1_metric\": f1_v ,\n",
    "                   \"Val/loss\": val_loss,\"train/accuracy\": train_accuracy,\n",
    "                   \"train/loss\": train_loss,'train/f1_metric':f1_t})\n",
    "        \n",
    "        if raytune_mode == True:\n",
    "            os.makedirs(\"checkpoint_models\", exist_ok=True)\n",
    "            torch.save(\n",
    "                        (model.state_dict(), optimizer.state_dict()), \"checkpoint_models/checkpoint.pt\")\n",
    "            checkpoint = Checkpoint.from_directory(\"checkpoint_models\")\n",
    "            train.report({\"loss\": val_loss, \"accuracy\": val_accuracy,\"f1\": f1_v}, checkpoint=checkpoint)\n",
    "        #if epoch%10 ==0:\n",
    "        #    print(\"Epoch:\", epoch)\n",
    "        #    \n",
    "        #    #print('Loss/train', train_loss.item(), epoch)\n",
    "        #    print(f'Accuracy/train{train_accuracy.item():0.6f}')\n",
    "        #    #print('Loss/val', val_loss.item(), epoch)\n",
    "        #    print(f'Accuracy/val{val_accuracy.item():0.6f}')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wjwlbqjl) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vital-sea-114</strong> at: <a href='https://wandb.ai/ustimovi/fight_detection_module/runs/wjwlbqjl' target=\"_blank\">https://wandb.ai/ustimovi/fight_detection_module/runs/wjwlbqjl</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231112_223558-wjwlbqjl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:wjwlbqjl). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\repositories\\Action_recognition_research\\Action_recognition_research\\wandb\\run-20231112_223944-x1kvpxjs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ustimovi/fight_detection_module/runs/x1kvpxjs' target=\"_blank\">peach-bird-115</a></strong> to <a href='https://wandb.ai/ustimovi/fight_detection_module' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ustimovi/fight_detection_module' target=\"_blank\">https://wandb.ai/ustimovi/fight_detection_module</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ustimovi/fight_detection_module/runs/x1kvpxjs' target=\"_blank\">https://wandb.ai/ustimovi/fight_detection_module/runs/x1kvpxjs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login(key = '66d1d2eaf7cd3f83b644fc151071bbf5d7f0c237')\n",
    "\n",
    "\n",
    "\n",
    "train_data = Keypoint_sequence_dataset(X_train,y_train)\n",
    "test_data = Keypoint_sequence_dataset(X_test,y_test)\n",
    "\n",
    "train_size = int(len(train_data) * 1)\n",
    "\n",
    "val_size = len(train_data) - train_size\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [train_size, val_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "#val_loader = torch.utils.data.DataLoader(val_data, batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "kernel_size = 3\n",
    "dropout_conv = 0.1\n",
    "dropout_linear = 0.1\n",
    "attention_size = 128\n",
    "attention_dropout = 0.1\n",
    "batch_size = 16\n",
    "attention_heads = 8\n",
    "conv_net = Seq_conv_net(kernel_size = kernel_size,dropout_conv = dropout_conv ,dropout_linear = dropout_linear,attention_size = attention_size,attention_heads=attention_heads,attention_dropout=attention_dropout)\n",
    "\n",
    "conv_net = conv_net.to(device)\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=loss_weights.to(device))\n",
    "\n",
    "# выбираем алгоритм оптимизации и learning_rate\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(conv_net.parameters(), lr=learning_rate)\n",
    "\n",
    "run = wandb.init(\n",
    "        # Set the project where this run will be logged\n",
    "        project=\"fight_detection_module\",\n",
    "        # Track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"learning_rate\": learning_rate,\n",
    "            'kernel_size':  kernel_size,\n",
    "            'dropout_conv': dropout_conv ,\n",
    "            'dropout_linear': dropout_linear ,\n",
    "            'Attention_size': attention_size ,\n",
    "            'Attention_dropout': attention_dropout,\n",
    "            'attention_heads': attention_heads,\n",
    "            'batch_size': batch_size    })\n",
    "\n",
    "\n",
    "conv_net = train_model(model=conv_net,loss_fn=loss_fn,optimizer=optimizer,train_loader=train_loader,val_loader=test_loader,n_epoch=300,raytune_mode= False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Tuning (AutoML analog in torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_hp = {\n",
    "    \"learning_rate\": tune.grid_search([0.01,0.001,0.0001]),\n",
    "    \"batch_size\": tune.grid_search([16, 32]),\n",
    "    \"kernel_size\":tune.grid_search([3,5,7]),\n",
    "    \"dropout_conv\": tune.grid_search([0.,0.1,0.2]),\n",
    "    \"dropout_linear\": tune.grid_search([0.,0.1,0.2]),\n",
    "    \"attention_size\": tune.grid_search([128,256]),\n",
    "    \"attention_dropout\": tune.grid_search([0.,0.1,0.2]),\n",
    "    \"attention_heads\": tune.grid_search([4,8,16]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Keypoint_sequence_dataset(X_train,y_train)\n",
    "test_data = Keypoint_sequence_dataset(X_test,y_test)\n",
    "\n",
    "def ray_train_model(config):\n",
    "\n",
    "    batch_size = config['batch_size']\n",
    "    kernel_size = config['kernel_size']\n",
    "    dropout_conv = config['dropout_conv']\n",
    "    dropout_linear = config['dropout_linear']\n",
    "    attention_size = config['attention_size']\n",
    "    attention_dropout = config['attention_dropout']\n",
    "    attention_heads = config['attention_heads']\n",
    "    learning_rate = config['learning_rate']\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=int(batch_size), shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=int(batch_size), shuffle=True)\n",
    "\n",
    "    conv_net = Seq_conv_net(kernel_size = kernel_size,dropout_conv = dropout_conv ,dropout_linear = dropout_linear,attention_size = attention_size,attention_heads=attention_heads,attention_dropout=attention_dropout)\n",
    "\n",
    "    conv_net = conv_net.to(device)\n",
    "\n",
    "\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight=loss_weights.to(device))\n",
    "\n",
    "    \n",
    "    optimizer = torch.optim.Adam(conv_net.parameters(), lr=learning_rate)\n",
    "\n",
    "    loaded_checkpoint = train.get_checkpoint()\n",
    "    if loaded_checkpoint:\n",
    "        with loaded_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
    "           model_state, optimizer_state = torch.load(os.path.join(loaded_checkpoint_dir, \"checkpoint.pt\"))\n",
    "        conv_net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    conv_net = train_model(model=conv_net,loss_fn=loss_fn,optimizer=optimizer,train_loader=train_loader,val_loader=test_loader,n_epoch=100,raytune_mode=True,wandb_mode = False)\n",
    "   \n",
    "def evaluate_best_model(net, dataset, loss_fn,best_result,config):\n",
    "\n",
    "    model = net(kernel_size = config['kernel_size'],dropout_conv = config['dropout_conv'] ,dropout_linear = config['dropout_linear'],\n",
    "                attention_size = config['attention_size'],attention_heads=config['attention_heads'],attention_dropout=config['attention_dropout'])\n",
    "    model.to(device)\n",
    "    checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "    model_state, optimizer_state = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(model_state)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight=loss_weights.to(device))\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=int(config[\"batch_size\"]), shuffle=True)\n",
    "\n",
    "    return evaluate_model(model,dataloader,loss_fn)\n",
    "\n",
    "def main(num_samples=1, max_num_epochs=100, gpus_per_trial=1):\n",
    "    config_hp = {\n",
    "    \"learning_rate\": tune.grid_search([0.0001]),\n",
    "    \"batch_size\": tune.grid_search([16]),\n",
    "    \"kernel_size\":tune.grid_search([5]),\n",
    "    \"dropout_conv\": tune.grid_search([0.,0.05]),\n",
    "    \"dropout_linear\": tune.grid_search([0.,0.05]),\n",
    "    \"attention_size\": tune.grid_search([128,160]),\n",
    "    \"attention_dropout\": tune.grid_search([0.,0.05]),\n",
    "    \"attention_heads\": tune.grid_search([16,32]),\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    \n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(ray_train_model),\n",
    "            resources={\"cpu\": 0, \"gpu\": gpus_per_trial}\n",
    "        ),\n",
    "        run_config=train.RunConfig(\n",
    "        name=\"fight-exp\",\n",
    "        local_dir=r\"D:\\ray_temp\",\n",
    "        checkpoint_config=train.CheckpointConfig(\n",
    "            checkpoint_score_attribute=\"accuracy\",\n",
    "            checkpoint_score_order=\"max\",\n",
    "            num_to_keep=5,\n",
    "        ),\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"accuracy\",\n",
    "            mode=\"max\",\n",
    "            scheduler=scheduler,\n",
    "            num_samples=num_samples,\n",
    "        ),\n",
    "        param_space=config_hp,\n",
    "    )\n",
    "\n",
    "    results = tuner.fit()\n",
    "\n",
    "    return results\n",
    "\n",
    "#results = main(num_samples=1, max_num_epochs=50, gpus_per_trial=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'learning_rate': 0.0001, 'batch_size': 16, 'kernel_size': 5, 'dropout_conv': 0.1, 'dropout_linear': 0.1, 'attention_size': 112, 'attention_dropout': 0.1, 'attention_heads': 16}\n",
      "Best trial final validation loss: 0.1770070381462574\n",
      "Best trial final validation accuracy: 0.8181818127632141\n",
      "Best trial f1 metric: 0.4444444477558136\n",
      "[[36  4]\n",
      " [11  4]]\n"
     ]
    }
   ],
   "source": [
    "best_result = results.get_best_result(\"loss\", \"min\")\n",
    "\n",
    "print(\"Best trial config: {}\".format(best_result.config))\n",
    "print(\"Best trial final validation loss: {}\".format(\n",
    "        best_result.metrics[\"loss\"]))\n",
    "print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_result.metrics[\"accuracy\"]))\n",
    "print(\"Best trial f1 metric: {}\".format(\n",
    "        best_result.metrics[\"f1\"]))\n",
    "    \n",
    "a,b,e,conf_matrix = evaluate_best_model(Seq_conv_net,test_data,torch.nn.CrossEntropyLoss(weight=loss_weights.to(device)),best_result,best_result.config)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "tuner = tune.Tuner.restore(\n",
    "    r\"D:\\ray_temp\",\n",
    "    trainable=my_trainable,\n",
    "    resume_errored=True\n",
    ")\n",
    "tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[378   5]\n",
      " [ 19  92]]\n",
      "[[39  3]\n",
      " [ 5  8]]\n"
     ]
    }
   ],
   "source": [
    "a,b,e,conf_matrix = evaluate_model(conv_net, train_loader, loss_fn)\n",
    "print(conf_matrix)\n",
    "a,b,e,conf_matrix = evaluate_model(conv_net, test_loader, loss_fn)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scripted = torch.jit.script(conv_net)\n",
    "model_scripted.save('fight_detection_v2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on a real video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_keypoints(keypoints):\n",
    "    seq_data = torch.transpose(torch.Tensor(keypoints),0,1)\n",
    "    data_shape = seq_data.shape\n",
    "    output_data = np.zeros((data_shape)) \n",
    "    for i,seq in enumerate(seq_data):\n",
    "        output_data[i] = approximate_linear(seq)\n",
    "\n",
    "    return torch.Tensor([output_data])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "modely = YOLO('yolov8l-pose.pt')  # load a pretrained YOLOv8n classification model\n",
    "modely.to(device)\n",
    "video_path = r\"D:\\videos\\hands3.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "# Get video properties\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) # or number\n",
    "# Create a VideoWriter object to save the output video\n",
    "output_video_path = r\"D:\\videos_processed\\fight1_processed.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "\n",
    "actions = ['fighting','not_fighting']\n",
    "\n",
    "text2 = \"No suspicious activity\"\n",
    "text1 = \"Suspicious activity\"\n",
    "text3 = \"No people in sight\"\n",
    "color_map = {'fighting': (200,100,0),'not_fighting': (0,100,200)}\n",
    "font_scale = 1.6\n",
    "thickness = 2\n",
    "\n",
    "winsize = 40\n",
    "\n",
    "distance_dict = {}\n",
    "\n",
    "\n",
    "label_map = {num: label for num, label in enumerate(actions)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1317, 0.8683]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0314, 0.9686]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0830, 0.9170]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.6340, 0.3660]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0755, 0.9245]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0766, 0.9234]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.6932, 0.3068]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0782, 0.9218]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3580, 0.6420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2321, 0.7679]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4379, 0.5621]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1167, 0.8833]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.8522, 0.1478]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.6053, 0.3947]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2368, 0.7632]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.9202, 0.0798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.6202, 0.3798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.7120, 0.2880]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0434, 0.9566]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.7264, 0.2736]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ans = 'not_fighting'\n",
    "while cap.isOpened():\n",
    "# Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "\n",
    "        results = modely.track(frame, persist=True, retina_masks=True, boxes=True, show_conf=False, line_width=1,  conf=0.6, iou=0.5,  classes=0, show_labels=False, device=device,verbose = False,tracker=\"bytetrack.yaml\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if results[0].boxes.id is not None:\n",
    "            \n",
    "            #extracting keypoints\n",
    "            body_kp = extract_body_keypoints(results = results,threshold_class=0.4,threshold_keypoint=0.4)\n",
    "            hands_kp = extract_hands_keypoints(results = results,threshold_class=0.4,threshold_keypoint=0.4)\n",
    "            head_kp = extract_head_keypoints(results = results,threshold_class=0.4,threshold_keypoint=0.4)\n",
    "            #calculating distances between keypoints\n",
    "\n",
    "            dd = calc_distances(hands_kp,body_kp,head_kp)\n",
    "            #appending distances dictionary and evaluating average distance and classification based on it\n",
    "            for key in dd.keys():\n",
    "\n",
    "                if key not in distance_dict.keys():\n",
    "                    distance_dict[key] = deque(maxlen=40)\n",
    "\n",
    "                distance_dict[key].append(dd[key])\n",
    "                \n",
    "                if len(distance_dict[key]) == winsize:\n",
    "                    nums_sequences = nums_sequences + 1\n",
    "                    keypoints = preprocess_keypoints(distance_dict[key])\n",
    "                    logits = conv_net(keypoints.to(device,dtype = torch.float))\n",
    "                    prediction = int(torch.argmax(logits, dim=1).cpu())\n",
    "                    print(logits)\n",
    "                    ans = label_map[prediction]\n",
    "                    distance_dict[key].clear()\n",
    "                    if ans == 'fighting':\n",
    "                        break\n",
    "                    \n",
    "\n",
    "            text_size, _ = cv2.getTextSize(ans, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)\n",
    "            text_position = (frame_width - text_size[0] - 10, text_size[1] + 10)\n",
    "            cv2.rectangle(frame, (text_position[0] - 5, text_position[1] - text_size[1] - 5),\n",
    "                                    (text_position[0] + text_size[0] + 5, text_position[1] + 5), color=(0, 0, 0),\n",
    "                                    thickness=cv2.FILLED)\n",
    "            cv2.putText(frame, ans, text_position, cv2.FONT_HERSHEY_SIMPLEX, font_scale, color_map[ans], thickness, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "                    \n",
    "            \n",
    "\n",
    "        annotated_frame_show = cv2.resize(frame, (1080, 720))\n",
    "        out.write(frame)\n",
    "        cv2.imshow(\"YOLOv8 Inference\", annotated_frame_show)\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
