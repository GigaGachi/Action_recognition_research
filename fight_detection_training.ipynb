{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from pydantic import BaseModel\n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_path = r'D:\\Datasets\\fight'\n",
    "actions = ['fighting','not_fighting']\n",
    "sequence_length = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "model = YOLO('yolov8x-pose.pt')  # load a pretrained YOLOv8n classification model\n",
    "model.to(device)\n",
    "video_path = r\"D:\\videos\\hands3.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "# Get video properties\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) # or number\n",
    "# Create a VideoWriter object to save the output video\n",
    "output_video_path = r\"D:\\videos_processed\\hands3_processed.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "\n",
    "def extract_keypoints(results, threshold_class, threshold_keypoint):\n",
    "    existing_kp = {}\n",
    "    for result,i_d in zip(results[0],results[0].boxes.id):\n",
    "        # There results for bounding boxes, and confidence scores for general detect\n",
    "        x1, y1, x2, y2,_, conf_for_detect, class_id_detected = (result.boxes.data.tolist())[0]\n",
    "        # If the confidence score for general detect is lower than threshold, skip\n",
    "        if conf_for_detect < threshold_class:\n",
    "            continue\n",
    "        # keypoints\n",
    "        keys = (result.keypoints.data.tolist())[0]\n",
    "        keyp_arr = list()\n",
    "        for key in keys:\n",
    "            keyp_arr.append(key)\n",
    "        # Adding existing hand keypoints of an object in a frame to the dictionary   \n",
    "        existing_kp[int(i_d)] = keyp_arr\n",
    "    return existing_kp\n",
    "\n",
    "def calc_kp_to_kp_dist(keypoints_dict):\n",
    "    # creating a dictionary of distances between each keypoint (except of the same object) in the keypoint_dict\n",
    "    dist_dict = {}\n",
    "    num_obj = len(keypoints_dict.keys())\n",
    "    keys = keypoints_dict.keys()\n",
    "    # calculating distances between keypoints \n",
    "    for l,keyi in enumerate(keys,start =1):\n",
    "        for m,keyj in enumerate(keys,start =1):\n",
    "            if m>=l:\n",
    "                break  \n",
    "            for i,p1 in enumerate(keypoints_dict[keyi]):\n",
    "                for j,p2 in enumerate(keypoints_dict[keyj]):\n",
    "                    dist = calc_euclid_dist(p1,p2)\n",
    "                    dist_dict[f'{keyi}'+f'{keyj}'+f'{i}'+f'{j}'] = dist\n",
    "    return dist_dict\n",
    "\n",
    "def calc_euclid_dist(p1,p2):\n",
    "    if (len(p1)>0) and (len(p2)>0):\n",
    "        dist = int(math.sqrt((p1[0]-p2[0])*(p1[0]-p2[0]) + (p1[1]-p2[1])*(p1[1]-p2[1])))\n",
    "        return dist\n",
    "    else: \n",
    "        return None\n",
    "    \n",
    "def calc_grad(dist_dict):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"No suspicious activity\"\n",
    "text1 = \"Suspicious activity\"\n",
    "text3 = \"No people in sight\"\n",
    "color2 = (100, 200, 0)\n",
    "color1 = (100, 0, 200)\n",
    "color3 = (100, 100, 100)\n",
    "font_scale = 1.6\n",
    "thickness = 2\n",
    "\n",
    "winsize = 60\n",
    "all_keypoints = {}\n",
    "distance_dict = {}\n",
    "average_dist = {}\n",
    "grad_dict = {}\n",
    "outputs = [0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while cap.isOpened():\n",
    "# Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "\n",
    "        results = model.track(frame, persist=True, retina_masks=True, boxes=True, show_conf=False, line_width=1,  conf=0.3, iou=0.5,  classes=0, show_labels=False, device=device,verbose = False,tracker=\"bytetrack.yaml\")\n",
    "        if results[0].boxes.id is not None:\n",
    "\n",
    "            #extracting keypoints\n",
    "            kp = extract_keypoints(results = results,threshold_class=0.2,threshold_keypoint=0.2)\n",
    "    \n",
    "            #appending keypoints to dictionary with size = winsize frames window\n",
    "\n",
    "            for i_d in results[0].boxes.id:\n",
    "                if int(i_d) not in all_keypoints.keys():\n",
    "                    all_keypoints[int(i_d)] = deque(maxlen=winsize)\n",
    "                all_keypoints[int(i_d)].append(kp[int(i_d)])\n",
    "\n",
    "            #calculating distances between keypoints\n",
    "\n",
    "            dd = calc_kp_to_kp_dist(kp)\n",
    "            #appending distances dictionary and evaluating average distance and classification based on it\n",
    "            for key in dd.keys():\n",
    "\n",
    "                if key not in distance_dict.keys():\n",
    "                    distance_dict[key] = deque(maxlen=winsize)\n",
    "\n",
    "                average_dist[key] = np.mean(distance_dict[key])\n",
    "                distance_dict[key].append(dd[key])\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "        annotated_frame_show = cv2.resize(frame, (1080, 720))\n",
    "        cv2.imshow(\"YOLOv8 Inference\", annotated_frame_show)\n",
    "        out.write(frame)\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
